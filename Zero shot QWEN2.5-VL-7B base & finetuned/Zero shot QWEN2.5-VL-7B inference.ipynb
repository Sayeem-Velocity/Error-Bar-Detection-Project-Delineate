{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af39a058",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T13:07:40.612585Z",
     "iopub.status.busy": "2026-01-28T13:07:40.612276Z",
     "iopub.status.idle": "2026-01-28T13:07:50.339657Z",
     "shell.execute_reply": "2026-01-28T13:07:50.338745Z",
     "shell.execute_reply.started": "2026-01-28T13:07:40.612558Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hLibraries installed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Install Required Libraries\n",
    "# Use bitsandbytes for 4-bit quantization\n",
    "!pip install transformers accelerate bitsandbytes -q\n",
    "!pip install pandas pillow tqdm -q\n",
    "print(\"Libraries installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d9066a",
   "metadata": {},
   "source": [
    "## 0. Install Required Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe348e0a",
   "metadata": {},
   "source": [
    "# Error Bar Detection using VQA + Chain-of-Thought Reasoning (4-bit Quantized)\n",
    "## Visual Question Answering with Step-by-Step CoT - Qwen2.5-VL\n",
    "\n",
    "**Objective:** Detect error bars in scientific charts using Visual Question Answering with explicit Chain-of-Thought reasoning for improved accuracy.\n",
    "\n",
    "**Approach:**\n",
    "- Load Qwen2.5-VL-7B-Instruct model with 4-bit quantization for efficiency\n",
    "- Break down detection into sequential VQA steps with CoT prompting\n",
    "- Step 1: Identify and verify data point marker location\n",
    "- Step 2: Scan upward with reasoning to find top error bar endpoint\n",
    "- Step 3: Scan downward with reasoning to find bottom error bar endpoint\n",
    "- Aggregate multi-step reasoning into final pixel measurements\n",
    "- Evaluation against ground truth annotations\n",
    "\n",
    "**Why VQA + CoT:**\n",
    "- Explicit reasoning improves measurement accuracy\n",
    "- Self-verification catches errors\n",
    "- Explainable outputs for debugging\n",
    "- Better handles ambiguous cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1ba64f",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63743fbe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T13:07:50.341787Z",
     "iopub.status.busy": "2026-01-28T13:07:50.341463Z",
     "iopub.status.idle": "2026-01-28T13:08:17.162449Z",
     "shell.execute_reply": "2026-01-28T13:08:17.161644Z",
     "shell.execute_reply.started": "2026-01-28T13:07:50.341758Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-28 13:08:03.748052: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1769605683.918516      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1769605683.964129      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1769605684.343966      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1769605684.344006      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1769605684.344008      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1769605684.344011      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Available: True\n",
      "GPU Name: Tesla T4\n",
      "GPU Memory: 15.8 GB\n",
      "\n",
      "Libraries imported successfully!\n",
      "PyTorch version: 2.8.0+cu126\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Import Libraries\n",
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "import gc\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For image processing\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# For model loading - using the same import as meme classification\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor\n",
    "\n",
    "# Check GPU\n",
    "print(f\"GPU Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"GPU Memory: {gpu_mem:.1f} GB\")\n",
    "\n",
    "print(\"\\nLibraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6510b172",
   "metadata": {},
   "source": [
    "## 2. Configuration and Data Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "801e15c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T13:08:17.163933Z",
     "iopub.status.busy": "2026-01-28T13:08:17.163367Z",
     "iopub.status.idle": "2026-01-28T13:08:17.170092Z",
     "shell.execute_reply": "2026-01-28T13:08:17.169339Z",
     "shell.execute_reply.started": "2026-01-28T13:08:17.163908Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Qwen/Qwen2.5-VL-7B-Instruct\n",
      "Max tokens: 1024\n",
      "Image max size: 768px\n",
      "Test images: /kaggle/input/graph-plots/Test/images\n",
      "Test input labels: /kaggle/input/graph-plots/Test/test_labels\n",
      "Ground truth: /kaggle/input/graph-plots/Test/labels\n"
     ]
    }
   ],
   "source": [
    "# Data paths (Kaggle format)\n",
    "BASE_PATH = \"/kaggle/input/graph-plots\"\n",
    "TEST_IMAGES = os.path.join(BASE_PATH, \"Test\", \"images\")\n",
    "TEST_INPUT_LABELS = os.path.join(BASE_PATH, \"Test\", \"test_labels\")  # Input: x,y only\n",
    "TEST_GROUND_TRUTH = os.path.join(BASE_PATH, \"Test\", \"labels\")       # Ground truth: with error bars\n",
    "\n",
    "# Model configuration - Optimized for fast inference\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-VL-7B-Instruct\"\n",
    "MAX_NEW_TOKENS = 1024  # Enough tokens for all data points\n",
    "TEMPERATURE = 0.1  # Low temperature for deterministic outputs\n",
    "IMAGE_MAX_SIZE = 768  # Max image dimension for faster processing\n",
    "\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Max tokens: {MAX_NEW_TOKENS}\")\n",
    "print(f\"Image max size: {IMAGE_MAX_SIZE}px\")\n",
    "print(f\"Test images: {TEST_IMAGES}\")\n",
    "print(f\"Test input labels: {TEST_INPUT_LABELS}\")\n",
    "print(f\"Ground truth: {TEST_GROUND_TRUTH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1031807f",
   "metadata": {},
   "source": [
    "## 3. Load Vision-Language Model (4-bit Quantization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56d7a15c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T13:08:17.172803Z",
     "iopub.status.busy": "2026-01-28T13:08:17.172255Z",
     "iopub.status.idle": "2026-01-28T13:11:09.084786Z",
     "shell.execute_reply": "2026-01-28T13:11:09.083870Z",
     "shell.execute_reply.started": "2026-01-28T13:08:17.172780Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "LOADING QWEN2.5-VL-7B-INSTRUCT (FP16)\n",
      "============================================================\n",
      "\n",
      "Loading model: Qwen/Qwen2.5-VL-7B-Instruct\n",
      "This may take 2-3 minutes...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2cfb11831ed4308a070a800c8ad7e53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58bbdb5325494f94a674bc37c701b55d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e293969ae0e45f1891e3f2ed97f7673",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c92db09e8a44757ba29882b75d4b0f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "956a343dbb7f4a1bac30939d1d99b6ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba448ca6daf5472bb2dc9bf85e48c328",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processor loaded!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "775cdad910c64c6587f965b4d84c3bc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bc91b3772d74bc5a9161dc042c8bdf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ca7fd1172ed41b19d04a9a04d4fd7f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9644b039df304e479bc87e01fe73fcd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00005.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "959055e1fa574aea877a825181e942d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00005.safetensors:   0%|          | 0.00/3.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "161baf67a1cc4c7fa780dc7fde7c0e71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00005.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "409411c5ca194916bf84b292ca1fcf28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00005.safetensors:   0%|          | 0.00/1.09G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f5ceba63dd644b1bf1f8ea5d5d60fa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00005.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7d2c8e38f704b88af889284f87f7d6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58715eea1189416b9d434e2c70bf1575",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/216 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded with FP16 precision!\n",
      "GPU Memory Used: 7.57 GB\n",
      "\n",
      "============================================================\n",
      "MODEL READY FOR INFERENCE\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Load Qwen2.5-VL Model (FP16 - Stable & Fast)\n",
    "# No quantization for stable vision embeddings\n",
    "\n",
    "import torch\n",
    "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
    "\n",
    "def load_qwen_model():\n",
    "    \"\"\"\n",
    "    Load Qwen2.5-VL-7B-Instruct with float16 precision.\n",
    "    More stable than 4-bit for vision tasks.\n",
    "    Uses ~14GB VRAM with FP16.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"LOADING QWEN2.5-VL-7B-INSTRUCT (FP16)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    MODEL_ID = \"Qwen/Qwen2.5-VL-7B-Instruct\"\n",
    "    \n",
    "    print(f\"\\nLoading model: {MODEL_ID}\")\n",
    "    print(\"This may take 2-3 minutes...\")\n",
    "    \n",
    "    # Load processor\n",
    "    processor = AutoProcessor.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "    print(\"Processor loaded!\")\n",
    "    \n",
    "    # Load model with FP16 (stable for vision)\n",
    "    model = AutoModelForVision2Seq.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    model.eval()\n",
    "    print(\"Model loaded with FP16 precision!\")\n",
    "    \n",
    "    # Print memory usage\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1e9\n",
    "        print(f\"GPU Memory Used: {allocated:.2f} GB\")\n",
    "    \n",
    "    return model, processor\n",
    "\n",
    "\n",
    "# Load the model\n",
    "model, processor = load_qwen_model()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL READY FOR INFERENCE\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c52be36",
   "metadata": {},
   "source": [
    "## 4. Define System and User Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ff44653",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T13:11:09.086522Z",
     "iopub.status.busy": "2026-01-28T13:11:09.086170Z",
     "iopub.status.idle": "2026-01-28T13:11:09.090991Z",
     "shell.execute_reply": "2026-01-28T13:11:09.090149Z",
     "shell.execute_reply.started": "2026-01-28T13:11:09.086487Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System prompt defined!\n"
     ]
    }
   ],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are analyzing scientific plots.\n",
    "\n",
    "Task:\n",
    "- Identify whether error bars are present.\n",
    "- Identify orientation (vertical or horizontal).\n",
    "- Identify whether error bars are symmetric or asymmetric.\n",
    "\n",
    "Rules:\n",
    "- Do NOT estimate pixel values.\n",
    "- Do NOT guess numeric distances.\n",
    "- Respond ONLY in valid JSON format.\n",
    "\"\"\"\n",
    "\n",
    "print(\"System prompt defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfeb88e6",
   "metadata": {},
   "source": [
    "## 5. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce3e016c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T13:11:09.092188Z",
     "iopub.status.busy": "2026-01-28T13:11:09.091884Z",
     "iopub.status.idle": "2026-01-28T13:11:09.110308Z",
     "shell.execute_reply": "2026-01-28T13:11:09.109616Z",
     "shell.execute_reply.started": "2026-01-28T13:11:09.092158Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions defined!\n"
     ]
    }
   ],
   "source": [
    "def load_test_input(json_path):\n",
    "    \"\"\"Load test input JSON (contains only x,y coordinates)\"\"\"\n",
    "    with open(json_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def load_ground_truth(json_path):\n",
    "    \"\"\"Load ground truth JSON (contains error bar distances)\"\"\"\n",
    "    with open(json_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def load_image_as_pil(image_path):\n",
    "    \"\"\"Load image as PIL Image\"\"\"\n",
    "    return Image.open(image_path).convert('RGB')\n",
    "\n",
    "def parse_vlm_response(response_text: str, original_points: List[Dict]) -> Optional[Dict]:\n",
    "    \"\"\"\n",
    "    Parse VLM response and convert to standard format.\n",
    "    Handles multiple output formats from the model.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Remove markdown code blocks if present\n",
    "        cleaned = response_text.strip()\n",
    "        if '```json' in cleaned:\n",
    "            start = cleaned.find('```json') + 7\n",
    "            end = cleaned.find('```', start)\n",
    "            if end > start:\n",
    "                cleaned = cleaned[start:end].strip()\n",
    "        elif '```' in cleaned:\n",
    "            start = cleaned.find('```') + 3\n",
    "            end = cleaned.find('```', start)\n",
    "            if end > start:\n",
    "                cleaned = cleaned[start:end].strip()\n",
    "        \n",
    "        # Find the JSON array or object\n",
    "        if cleaned.startswith('['):\n",
    "            json_str = cleaned\n",
    "        elif cleaned.startswith('{'):\n",
    "            json_str = cleaned\n",
    "        else:\n",
    "            start_idx = cleaned.find('[')\n",
    "            if start_idx < 0:\n",
    "                start_idx = cleaned.find('{')\n",
    "            end_idx = cleaned.rfind(']') + 1\n",
    "            if end_idx <= start_idx:\n",
    "                end_idx = cleaned.rfind('}') + 1\n",
    "            if start_idx >= 0 and end_idx > start_idx:\n",
    "                json_str = cleaned[start_idx:end_idx]\n",
    "            else:\n",
    "                print(\"No JSON found in response\")\n",
    "                return None\n",
    "        \n",
    "        # Parse JSON\n",
    "        parsed = json.loads(json_str)\n",
    "        \n",
    "        # Convert to list if single object\n",
    "        if isinstance(parsed, dict):\n",
    "            parsed = [parsed]\n",
    "        \n",
    "        # Convert to standard format\n",
    "        measurements = []\n",
    "        for i, item in enumerate(parsed):\n",
    "            # Handle different key formats the model might output\n",
    "            if 'data_point_x' in item:\n",
    "                # Flat format: data_point_x, data_point_y, upper_error_bar_y, lower_error_bar_y\n",
    "                x = float(item.get('data_point_x', item.get('x', 0)))\n",
    "                y = float(item.get('data_point_y', item.get('y', 0)))\n",
    "                upper_y = float(item.get('upper_error_bar_y', item.get('upper_y', y)))\n",
    "                lower_y = float(item.get('lower_error_bar_y', item.get('lower_y', y)))\n",
    "            elif 'x' in item and 'upper_y' in item:\n",
    "                # Simple format: x, y, upper_y, lower_y\n",
    "                x = float(item['x'])\n",
    "                y = float(item['y'])\n",
    "                upper_y = float(item.get('upper_y', y))\n",
    "                lower_y = float(item.get('lower_y', y))\n",
    "            elif 'data_point' in item:\n",
    "                # Nested format: data_point: {x, y}, upper_error_bar: {x, y}, lower_error_bar: {x, y}\n",
    "                x = float(item['data_point']['x'])\n",
    "                y = float(item['data_point']['y'])\n",
    "                upper_y = float(item.get('upper_error_bar', {}).get('y', y))\n",
    "                lower_y = float(item.get('lower_error_bar', {}).get('y', y))\n",
    "            else:\n",
    "                # Fallback - use original point if available\n",
    "                if i < len(original_points):\n",
    "                    x = float(original_points[i].get('x', 0))\n",
    "                    y = float(original_points[i].get('y', 0))\n",
    "                    upper_y = y\n",
    "                    lower_y = y\n",
    "                else:\n",
    "                    continue\n",
    "            \n",
    "            measurements.append({\n",
    "                \"data_point\": {\"x\": x, \"y\": y},\n",
    "                \"upper_error_bar\": {\"x\": x, \"y\": upper_y},\n",
    "                \"lower_error_bar\": {\"x\": x, \"y\": lower_y}\n",
    "            })\n",
    "        \n",
    "        return {\"measurements\": measurements}\n",
    "        \n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"JSON parsing error: {e}\")\n",
    "        print(f\"Response excerpt: {response_text[:500]}\")\n",
    "        \n",
    "        # Try regex extraction as fallback\n",
    "        try:\n",
    "            import re\n",
    "            measurements = []\n",
    "            \n",
    "            # Pattern for flat format\n",
    "            flat_pattern = r'\"(?:data_point_)?x\"?\\s*:\\s*([\\d.]+).*?\"(?:data_point_)?y\"?\\s*:\\s*([\\d.]+).*?\"(?:upper_error_bar_y|upper_y)\"?\\s*:\\s*([\\d.]+).*?\"(?:lower_error_bar_y|lower_y)\"?\\s*:\\s*([\\d.]+)'\n",
    "            matches = re.findall(flat_pattern, response_text, re.DOTALL)\n",
    "            \n",
    "            if matches:\n",
    "                for match in matches:\n",
    "                    measurements.append({\n",
    "                        \"data_point\": {\"x\": float(match[0]), \"y\": float(match[1])},\n",
    "                        \"upper_error_bar\": {\"x\": float(match[0]), \"y\": float(match[2])},\n",
    "                        \"lower_error_bar\": {\"x\": float(match[0]), \"y\": float(match[3])}\n",
    "                    })\n",
    "                print(f\"Extracted {len(measurements)} measurements using regex\")\n",
    "                return {\"measurements\": measurements}\n",
    "        except Exception as regex_error:\n",
    "            print(f\"Regex extraction failed: {regex_error}\")\n",
    "        \n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error parsing response: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"Helper functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e64aab1",
   "metadata": {},
   "source": [
    "## 6. Model Inference Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ec6c97c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T13:11:09.111564Z",
     "iopub.status.busy": "2026-01-28T13:11:09.111253Z",
     "iopub.status.idle": "2026-01-28T13:11:09.125097Z",
     "shell.execute_reply": "2026-01-28T13:11:09.124253Z",
     "shell.execute_reply.started": "2026-01-28T13:11:09.111526Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structure inference function defined!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from PIL import Image\n",
    "\n",
    "def infer_plot_structure(image_path: str) -> Optional[Dict]:\n",
    "    \"\"\"\n",
    "    Analyze plot structure without pixel estimation.\n",
    "    Returns JSON describing error bar presence and characteristics.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        \n",
    "        # Resize for consistency\n",
    "        max_size = 768\n",
    "        if max(image.size) > max_size:\n",
    "            ratio = max_size / max(image.size)\n",
    "            new_size = (int(image.size[0] * ratio), int(image.size[1] * ratio))\n",
    "            image = image.resize(new_size, Image.BILINEAR)\n",
    "        \n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": SYSTEM_PROMPT\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"image\", \"image\": image},\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": \"Analyze the plot and report error bar structure in JSON format.\"\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # Apply chat template\n",
    "        text = processor.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        \n",
    "        # Process inputs\n",
    "        inputs = processor(\n",
    "            text=[text],\n",
    "            images=[image],\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(model.device)\n",
    "        \n",
    "        # Generate response\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=120,\n",
    "                do_sample=False,\n",
    "                temperature=0.0,\n",
    "                pad_token_id=processor.tokenizer.pad_token_id\n",
    "            )\n",
    "        \n",
    "        # Decode\n",
    "        decoded = processor.batch_decode(\n",
    "            output,\n",
    "            skip_special_tokens=True\n",
    "        )[0]\n",
    "        \n",
    "        # Extract JSON safely\n",
    "        match = re.search(r\"\\{.*\\}\", decoded, re.DOTALL)\n",
    "        if not match:\n",
    "            print(f\"No JSON found in response: {decoded[:200]}\")\n",
    "            return None\n",
    "        \n",
    "        result = json.loads(match.group())\n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Structure inference failed: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"Structure inference function defined!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "225c0e0f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T13:11:09.126609Z",
     "iopub.status.busy": "2026-01-28T13:11:09.126200Z",
     "iopub.status.idle": "2026-01-28T13:11:12.514009Z",
     "shell.execute_reply": "2026-01-28T13:11:12.513118Z",
     "shell.execute_reply.started": "2026-01-28T13:11:09.126574Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenCV pixel measurement function defined!\n"
     ]
    }
   ],
   "source": [
    "# Install OpenCV for pixel measurement\n",
    "!pip install opencv-python -q\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def measure_error_bars_pixels(image_path: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Measure error bars using OpenCV edge detection.\n",
    "    Returns actual pixel measurements from the image.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read image in grayscale\n",
    "        img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if img is None:\n",
    "            print(f\"Failed to load image: {image_path}\")\n",
    "            return []\n",
    "        \n",
    "        # Edge detection\n",
    "        edges = cv2.Canny(img, 50, 150)\n",
    "        \n",
    "        # Detect vertical lines (error bars)\n",
    "        lines = cv2.HoughLinesP(\n",
    "            edges,\n",
    "            rho=1,\n",
    "            theta=np.pi / 180,\n",
    "            threshold=100,\n",
    "            minLineLength=30,\n",
    "            maxLineGap=5\n",
    "        )\n",
    "        \n",
    "        if lines is None:\n",
    "            return []\n",
    "        \n",
    "        error_bars = []\n",
    "        \n",
    "        for line in lines:\n",
    "            x1, y1, x2, y2 = line[0]\n",
    "            \n",
    "            # Vertical-ish line (error bar candidate)\n",
    "            if abs(x1 - x2) < 3:\n",
    "                length = abs(y2 - y1)\n",
    "                error_bars.append({\n",
    "                    \"x\": int(x1),\n",
    "                    \"top\": int(min(y1, y2)),\n",
    "                    \"bottom\": int(max(y1, y2)),\n",
    "                    \"pixel_length\": int(length)\n",
    "                })\n",
    "        \n",
    "        return error_bars\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Pixel measurement failed: {e}\")\n",
    "        return []\n",
    "\n",
    "print(\"OpenCV pixel measurement function defined!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6385153",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T13:11:12.516258Z",
     "iopub.status.busy": "2026-01-28T13:11:12.515564Z",
     "iopub.status.idle": "2026-01-28T13:11:12.521043Z",
     "shell.execute_reply": "2026-01-28T13:11:12.520131Z",
     "shell.execute_reply.started": "2026-01-28T13:11:12.516225Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined analysis pipeline defined!\n"
     ]
    }
   ],
   "source": [
    "def analyze_plot(image_path: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Combined pipeline: structure analysis + pixel measurement.\n",
    "    \"\"\"\n",
    "    structure = infer_plot_structure(image_path)\n",
    "    pixel_bars = measure_error_bars_pixels(image_path)\n",
    "    \n",
    "    return {\n",
    "        \"structure\": structure,\n",
    "        \"pixel_measurements\": pixel_bars\n",
    "    }\n",
    "\n",
    "print(\"Combined analysis pipeline defined!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cce9c5",
   "metadata": {},
   "source": [
    "## 7. Convert VLM Output to Standard Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ed1d1fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T13:11:12.523948Z",
     "iopub.status.busy": "2026-01-28T13:11:12.523711Z",
     "iopub.status.idle": "2026-01-28T13:11:12.535016Z",
     "shell.execute_reply": "2026-01-28T13:11:12.534348Z",
     "shell.execute_reply.started": "2026-01-28T13:11:12.523928Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Format conversion functions defined!\n"
     ]
    }
   ],
   "source": [
    "def convert_vlm_to_standard_format(result: Dict, line_name: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Convert VLM measurements to standard prediction format with pixel distances.\n",
    "    \"\"\"\n",
    "    points = []\n",
    "    \n",
    "    measurements = result.get('measurements', [])\n",
    "    \n",
    "    for measure in measurements:\n",
    "        data_pt = measure['data_point']\n",
    "        upper_bar = measure['upper_error_bar']\n",
    "        lower_bar = measure['lower_error_bar']\n",
    "        \n",
    "        x = data_pt['x']\n",
    "        y = data_pt['y']\n",
    "        \n",
    "        # Calculate pixel distances\n",
    "        top_dist = abs(y - upper_bar['y'])  # Distance to upper error bar\n",
    "        bottom_dist = abs(lower_bar['y'] - y)  # Distance to lower error bar\n",
    "        dev_dist = max(top_dist, bottom_dist)\n",
    "        \n",
    "        points.append({\n",
    "            \"x\": x,\n",
    "            \"y\": y,\n",
    "            \"label\": \"\",\n",
    "            \"topBarPixelDistance\": float(top_dist),\n",
    "            \"bottomBarPixelDistance\": float(bottom_dist),\n",
    "            \"deviationPixelDistance\": float(dev_dist)\n",
    "        })\n",
    "    \n",
    "    return {\n",
    "        \"label\": {\"lineName\": line_name},\n",
    "        \"points\": points\n",
    "    }\n",
    "\n",
    "def convert_to_output_format(image_file: str, predictions: List[Dict]) -> Dict:\n",
    "    \"\"\"\n",
    "    Convert to final output format with error bar endpoints.\n",
    "    \"\"\"\n",
    "    error_bars = []\n",
    "    \n",
    "    for pred_line in predictions:\n",
    "        line_name = pred_line.get('label', {}).get('lineName', '')\n",
    "        pred_points = [p for p in pred_line.get('points', []) \n",
    "                      if p.get('label', '') not in ['xmin', 'xmax', 'ymin', 'ymax']]\n",
    "        \n",
    "        points_data = []\n",
    "        for point in pred_points:\n",
    "            x = point['x']\n",
    "            y = point['y']\n",
    "            top_dist = point['topBarPixelDistance']\n",
    "            bottom_dist = point['bottomBarPixelDistance']\n",
    "            \n",
    "            point_data = {\n",
    "                \"data_point\": {\"x\": x, \"y\": y},\n",
    "                \"upper_error_bar\": {\"x\": x, \"y\": y - top_dist},\n",
    "                \"lower_error_bar\": {\"x\": x, \"y\": y + bottom_dist}\n",
    "            }\n",
    "            \n",
    "            points_data.append(point_data)\n",
    "        \n",
    "        line_data = {\n",
    "            \"lineName\": line_name,\n",
    "            \"points\": points_data\n",
    "        }\n",
    "        \n",
    "        error_bars.append(line_data)\n",
    "    \n",
    "    return {\n",
    "        \"image_file\": image_file,\n",
    "        \"model\": \"VQA-4bit-Fast\",\n",
    "        \"error_bars\": error_bars\n",
    "    }\n",
    "\n",
    "print(\"Format conversion functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653fa789",
   "metadata": {},
   "source": [
    "## 8. Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "195e6c1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T13:11:12.536732Z",
     "iopub.status.busy": "2026-01-28T13:11:12.536007Z",
     "iopub.status.idle": "2026-01-28T13:11:12.551372Z",
     "shell.execute_reply": "2026-01-28T13:11:12.550736Z",
     "shell.execute_reply.started": "2026-01-28T13:11:12.536709Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation functions defined!\n"
     ]
    }
   ],
   "source": [
    "def calculate_point_error(pred_point, gt_point):\n",
    "    \"\"\"Calculate absolute pixel error for a single point.\"\"\"\n",
    "    top_error = abs(pred_point.get('topBarPixelDistance', 0) - \n",
    "                   gt_point.get('topBarPixelDistance', 0))\n",
    "    bottom_error = abs(pred_point.get('bottomBarPixelDistance', 0) - \n",
    "                      gt_point.get('bottomBarPixelDistance', 0))\n",
    "    dev_error = abs(pred_point.get('deviationPixelDistance', 0) - \n",
    "                   gt_point.get('deviationPixelDistance', 0))\n",
    "    \n",
    "    return {\n",
    "        'top_error': top_error,\n",
    "        'bottom_error': bottom_error,\n",
    "        'deviation_error': dev_error,\n",
    "        'mean_error': (top_error + bottom_error) / 2\n",
    "    }\n",
    "\n",
    "def evaluate_predictions(predictions, ground_truth):\n",
    "    \"\"\"Evaluate predictions against ground truth.\"\"\"\n",
    "    all_errors = []\n",
    "    \n",
    "    for pred_line in predictions:\n",
    "        gt_line = None\n",
    "        for gt in ground_truth:\n",
    "            if gt.get('label', {}).get('lineName') == pred_line.get('label', {}).get('lineName'):\n",
    "                gt_line = gt\n",
    "                break\n",
    "        \n",
    "        if gt_line is None:\n",
    "            continue\n",
    "        \n",
    "        pred_points = [p for p in pred_line.get('points', []) \n",
    "                      if p.get('label', '') not in ['xmin', 'xmax', 'ymin', 'ymax']]\n",
    "        gt_points = [p for p in gt_line.get('points', []) \n",
    "                    if p.get('label', '') not in ['xmin', 'xmax', 'ymin', 'ymax']]\n",
    "        \n",
    "        for pred_pt, gt_pt in zip(pred_points, gt_points):\n",
    "            error = calculate_point_error(pred_pt, gt_pt)\n",
    "            all_errors.append(error)\n",
    "    \n",
    "    if not all_errors:\n",
    "        return None\n",
    "    \n",
    "    metrics = {\n",
    "        'num_points': len(all_errors),\n",
    "        'mean_top_error': np.mean([e['top_error'] for e in all_errors]),\n",
    "        'mean_bottom_error': np.mean([e['bottom_error'] for e in all_errors]),\n",
    "        'mean_deviation_error': np.mean([e['deviation_error'] for e in all_errors]),\n",
    "        'mean_overall_error': np.mean([e['mean_error'] for e in all_errors]),\n",
    "        'median_top_error': np.median([e['top_error'] for e in all_errors]),\n",
    "        'median_bottom_error': np.median([e['bottom_error'] for e in all_errors]),\n",
    "        'std_top_error': np.std([e['top_error'] for e in all_errors]),\n",
    "        'std_bottom_error': np.std([e['bottom_error'] for e in all_errors]),\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "print(\"Evaluation functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91c8de5",
   "metadata": {},
   "source": [
    "## 9. Test on Sample Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10147af5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T13:11:12.552730Z",
     "iopub.status.busy": "2026-01-28T13:11:12.552339Z",
     "iopub.status.idle": "2026-01-28T13:11:19.075488Z",
     "shell.execute_reply": "2026-01-28T13:11:19.074815Z",
     "shell.execute_reply.started": "2026-01-28T13:11:12.552708Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on sample: 00271e61-86e3-453f-8101-fe906ae927eb.json\n",
      "\n",
      "Analyzing image: 00271e61-86e3-453f-8101-fe906ae927eb.png\n",
      "\n",
      "=== Structure Analysis ===\n",
      "{\n",
      "  \"errorBarsPresent\": true,\n",
      "  \"orientation\": \"vertical\",\n",
      "  \"symmetry\": \"asymmetric\"\n",
      "}\n",
      "\n",
      "=== Pixel Measurements ===\n",
      "Detected 8 vertical lines\n",
      "\n",
      "First 3 detections:\n",
      "  x=66, top=8, bottom=449, length=441px\n",
      "  x=64, top=8, bottom=451, length=443px\n",
      "  x=96, top=10, bottom=204, length=194px\n"
     ]
    }
   ],
   "source": [
    "# Test on one sample image\n",
    "sample_json_files = sorted([f for f in os.listdir(TEST_INPUT_LABELS) if f.endswith('.json')])[:1]\n",
    "\n",
    "if sample_json_files:\n",
    "    sample_json = sample_json_files[0]\n",
    "    print(f\"Testing on sample: {sample_json}\\n\")\n",
    "    \n",
    "    # Load input\n",
    "    input_json = load_test_input(os.path.join(TEST_INPUT_LABELS, sample_json))\n",
    "    image_file = input_json['image_file']\n",
    "    image_path = os.path.join(TEST_IMAGES, image_file)\n",
    "    \n",
    "    print(f\"Analyzing image: {image_file}\")\n",
    "    \n",
    "    # Analyze using combined pipeline\n",
    "    result = analyze_plot(image_path)\n",
    "    \n",
    "    if result:\n",
    "        print(\"\\n=== Structure Analysis ===\")\n",
    "        print(json.dumps(result['structure'], indent=2))\n",
    "        \n",
    "        print(f\"\\n=== Pixel Measurements ===\")\n",
    "        print(f\"Detected {len(result['pixel_measurements'])} vertical lines\")\n",
    "        if result['pixel_measurements']:\n",
    "            print(\"\\nFirst 3 detections:\")\n",
    "            for bar in result['pixel_measurements'][:3]:\n",
    "                print(f\"  x={bar['x']}, top={bar['top']}, bottom={bar['bottom']}, length={bar['pixel_length']}px\")\n",
    "    else:\n",
    "        print(\"Analysis failed - no results\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf23475",
   "metadata": {},
   "source": [
    "## 10. Process All Test Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "260efac7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T13:11:19.076753Z",
     "iopub.status.busy": "2026-01-28T13:11:19.076492Z",
     "iopub.status.idle": "2026-01-28T13:41:22.326051Z",
     "shell.execute_reply": "2026-01-28T13:41:22.325210Z",
     "shell.execute_reply.started": "2026-01-28T13:11:19.076731Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 600 test images...\n",
      "\n",
      "======================================================================\n",
      "✓ 10 images processed\n",
      "Predictions stored: 10 files\n",
      "Failed: 0 images\n",
      "Total processed: 10 images\n",
      "======================================================================\n",
      "======================================================================\n",
      "✓ 20 images processed\n",
      "Predictions stored: 20 files\n",
      "Failed: 0 images\n",
      "Total processed: 20 images\n",
      "======================================================================\n",
      "======================================================================\n",
      "✓ 30 images processed\n",
      "Predictions stored: 30 files\n",
      "Failed: 0 images\n",
      "Total processed: 30 images\n",
      "======================================================================\n",
      "======================================================================\n",
      "✓ 40 images processed\n",
      "Predictions stored: 40 files\n",
      "Failed: 0 images\n",
      "Total processed: 40 images\n",
      "======================================================================\n",
      "======================================================================\n",
      "✓ 50 images processed\n",
      "Predictions stored: 50 files\n",
      "Failed: 0 images\n",
      "Total processed: 50 images\n",
      "======================================================================\n",
      "======================================================================\n",
      "✓ 60 images processed\n",
      "Predictions stored: 60 files\n",
      "Failed: 0 images\n",
      "Total processed: 60 images\n",
      "======================================================================\n",
      "======================================================================\n",
      "✓ 70 images processed\n",
      "Predictions stored: 70 files\n",
      "Failed: 0 images\n",
      "Total processed: 70 images\n",
      "======================================================================\n",
      "======================================================================\n",
      "✓ 80 images processed\n",
      "Predictions stored: 80 files\n",
      "Failed: 0 images\n",
      "Total processed: 80 images\n",
      "======================================================================\n",
      "======================================================================\n",
      "✓ 90 images processed\n",
      "Predictions stored: 90 files\n",
      "Failed: 0 images\n",
      "Total processed: 90 images\n",
      "======================================================================\n",
      "======================================================================\n",
      "✓ 100 images processed\n",
      "Predictions stored: 100 files\n",
      "Failed: 0 images\n",
      "Total processed: 100 images\n",
      "======================================================================\n",
      "======================================================================\n",
      "✓ 110 images processed\n",
      "Predictions stored: 110 files\n",
      "Failed: 0 images\n",
      "Total processed: 110 images\n",
      "======================================================================\n",
      "======================================================================\n",
      "✓ 120 images processed\n",
      "Predictions stored: 120 files\n",
      "Failed: 0 images\n",
      "Total processed: 120 images\n",
      "======================================================================\n",
      "======================================================================\n",
      "✓ 130 images processed\n",
      "Predictions stored: 130 files\n",
      "Failed: 0 images\n",
      "Total processed: 130 images\n",
      "======================================================================\n",
      "======================================================================\n",
      "✓ 140 images processed\n",
      "Predictions stored: 140 files\n",
      "Failed: 0 images\n",
      "Total processed: 140 images\n",
      "======================================================================\n",
      "======================================================================\n",
      "✓ 150 images processed\n",
      "Predictions stored: 150 files\n",
      "Failed: 0 images\n",
      "Total processed: 150 images\n",
      "======================================================================\n",
      "======================================================================\n",
      "✓ 160 images processed\n",
      "Predictions stored: 160 files\n",
      "Failed: 0 images\n",
      "Total processed: 160 images\n",
      "======================================================================\n",
      "======================================================================\n",
      "✓ 170 images processed\n",
      "Predictions stored: 170 files\n",
      "Failed: 0 images\n",
      "Total processed: 170 images\n",
      "======================================================================\n",
      "ERROR: Structure inference failed: Expecting ',' delimiter: line 22 column 6 (char 398)\n",
      "======================================================================\n",
      "✓ 180 images processed\n",
      "Predictions stored: 180 files\n",
      "Failed: 0 images\n",
      "Total processed: 180 images\n",
      "======================================================================\n",
      "======================================================================\n",
      "✓ 190 images processed\n",
      "Predictions stored: 190 files\n",
      "Failed: 0 images\n",
      "Total processed: 190 images\n",
      "======================================================================\n",
      "======================================================================\n",
      "✓ 200 images processed\n",
      "Predictions stored: 200 files\n",
      "Failed: 0 images\n",
      "Total processed: 200 images\n",
      "======================================================================\n",
      "======================================================================\n",
      "✓ 210 images processed\n",
      "Predictions stored: 210 files\n",
      "Failed: 0 images\n",
      "Total processed: 210 images\n",
      "======================================================================\n",
      "======================================================================\n",
      "✓ 220 images processed\n",
      "Predictions stored: 220 files\n",
      "Failed: 0 images\n",
      "Total processed: 220 images\n",
      "======================================================================\n",
      "======================================================================\n",
      "✓ 230 images processed\n",
      "Predictions stored: 230 files\n",
      "Failed: 0 images\n",
      "Total processed: 230 images\n",
      "======================================================================\n",
      "======================================================================\n",
      "✓ 240 images processed\n",
      "Predictions stored: 240 files\n",
      "Failed: 0 images\n",
      "Total processed: 240 images\n",
      "======================================================================\n",
      "======================================================================\n",
      "✓ 250 images processed\n",
      "Predictions stored: 250 files\n",
      "Failed: 0 images\n",
      "Total processed: 250 images\n",
      "======================================================================\n",
      "======================================================================\n",
      "✓ 260 images processed\n",
      "Predictions stored: 260 files\n",
      "Failed: 0 images\n",
      "Total processed: 260 images\n",
      "======================================================================\n",
      "======================================================================\n",
      "✓ 270 images processed\n",
      "Predictions stored: 270 files\n",
      "Failed: 0 images\n",
      "Total processed: 270 images\n",
      "======================================================================\n",
      "======================================================================\n",
      "✓ 280 images processed\n",
      "Predictions stored: 280 files\n",
      "Failed: 0 images\n",
      "Total processed: 280 images\n",
      "======================================================================\n",
      "======================================================================\n",
      "✓ 290 images processed\n",
      "Predictions stored: 290 files\n",
      "Failed: 0 images\n",
      "Total processed: 290 images\n",
      "======================================================================\n",
      "======================================================================\n",
      "✓ 300 images processed\n",
      "Predictions stored: 300 files\n",
      "Failed: 0 images\n",
      "Total processed: 300 images\n",
      "======================================================================\n",
      "======================================================================\n",
      "✓ 310 images processed\n",
      "Predictions stored: 310 files\n",
      "Failed: 0 images\n",
      "Total processed: 310 images\n",
      "======================================================================\n",
      "======================================================================\n",
      "✓ 320 images processed\n",
      "Predictions stored: 320 files\n",
      "Failed: 0 images\n",
      "Total processed: 320 images\n",
      "======================================================================\n",
      "======================================================================\n",
      "✓ 330 images processed\n",
      "Predictions stored: 330 files\n",
      "Failed: 0 images\n",
      "Total processed: 330 images\n",
      "======================================================================\n",
      "======================================================================\n",
      "✓ 340 images processed\n",
      "Predictions stored: 340 files\n",
      "Failed: 0 images\n",
      "Total processed: 340 images\n",
      "======================================================================\n",
      "======================================================================\n",
      "✓ 350 images processed\n",
      "Predictions stored: 350 files\n",
      "Failed: 0 images\n",
      "Total processed: 350 images\n",
      "======================================================================\n",
      "======================================================================\n",
      "✓ 360 images processed\n",
      "Predictions stored: 360 files\n",
      "Failed: 0 images\n",
      "Total processed: 360 images\n",
      "======================================================================\n",
      "======================================================================\n",
      "✓ 370 images processed\n",
      "Predictions stored: 370 files\n",
      "Failed: 0 images\n",
      "Total processed: 370 images\n",
      "======================================================================\n",
      "======================================================================\n",
      "✓ 380 images processed\n",
      "Predictions stored: 380 files\n",
      "Failed: 0 images\n",
      "Total processed: 380 images\n",
      "======================================================================\n",
      "======================================================================\n",
      "✓ 390 images processed\n",
      "Predictions stored: 390 files\n",
      "Failed: 0 images\n",
      "Total processed: 390 images\n",
      "======================================================================\n",
      "======================================================================\n",
      "✓ 400 images processed\n",
      "Predictions stored: 400 files\n",
      "Failed: 0 images\n",
      "Total processed: 400 images\n",
      "======================================================================\n",
      "======================================================================\n",
      "✓ 410 images processed\n",
      "Predictions stored: 410 files\n",
      "Failed: 0 images\n",
      "Total processed: 410 images\n",
      "======================================================================\n",
      "======================================================================\n",
      "✓ 420 images processed\n",
      "Predictions stored: 420 files\n",
      "Failed: 0 images\n",
      "Total processed: 420 images\n",
      "======================================================================\n",
      "======================================================================\n",
      "✓ 430 images processed\n",
      "Predictions stored: 430 files\n",
      "Failed: 0 images\n",
      "Total processed: 430 images\n",
      "======================================================================\n",
      "======================================================================\n",
      "✓ 440 images processed\n",
      "Predictions stored: 440 files\n",
      "Failed: 0 images\n",
      "Total processed: 440 images\n",
      "======================================================================\n",
      "======================================================================\n",
      "✓ 450 images processed\n",
      "Predictions stored: 450 files\n",
      "Failed: 0 images\n",
      "Total processed: 450 images\n",
      "======================================================================\n",
      "======================================================================\n",
      "✓ 460 images processed\n",
      "Predictions stored: 460 files\n",
      "Failed: 0 images\n",
      "Total processed: 460 images\n",
      "======================================================================\n",
      "======================================================================\n",
      "✓ 470 images processed\n",
      "Predictions stored: 470 files\n",
      "Failed: 0 images\n",
      "Total processed: 470 images\n",
      "======================================================================\n",
      "ERROR: Structure inference failed: Expecting ',' delimiter: line 22 column 6 (char 398)\n",
      "======================================================================\n",
      "✓ 480 images processed\n",
      "Predictions stored: 480 files\n",
      "Failed: 0 images\n",
      "Total processed: 480 images\n",
      "======================================================================\n",
      "======================================================================\n",
      "✓ 490 images processed\n",
      "Predictions stored: 490 files\n",
      "Failed: 0 images\n",
      "Total processed: 490 images\n",
      "======================================================================\n",
      "======================================================================\n",
      "✓ 500 images processed\n",
      "Predictions stored: 500 files\n",
      "Failed: 0 images\n",
      "Total processed: 500 images\n",
      "======================================================================\n",
      "======================================================================\n",
      "✓ 510 images processed\n",
      "Predictions stored: 510 files\n",
      "Failed: 0 images\n",
      "Total processed: 510 images\n",
      "======================================================================\n",
      "======================================================================\n",
      "✓ 520 images processed\n",
      "Predictions stored: 520 files\n",
      "Failed: 0 images\n",
      "Total processed: 520 images\n",
      "======================================================================\n",
      "======================================================================\n",
      "✓ 530 images processed\n",
      "Predictions stored: 530 files\n",
      "Failed: 0 images\n",
      "Total processed: 530 images\n",
      "======================================================================\n",
      "======================================================================\n",
      "✓ 540 images processed\n",
      "Predictions stored: 540 files\n",
      "Failed: 0 images\n",
      "Total processed: 540 images\n",
      "======================================================================\n",
      "======================================================================\n",
      "✓ 550 images processed\n",
      "Predictions stored: 550 files\n",
      "Failed: 0 images\n",
      "Total processed: 550 images\n",
      "======================================================================\n",
      "======================================================================\n",
      "✓ 560 images processed\n",
      "Predictions stored: 560 files\n",
      "Failed: 0 images\n",
      "Total processed: 560 images\n",
      "======================================================================\n",
      "======================================================================\n",
      "✓ 570 images processed\n",
      "Predictions stored: 570 files\n",
      "Failed: 0 images\n",
      "Total processed: 570 images\n",
      "======================================================================\n",
      "======================================================================\n",
      "✓ 580 images processed\n",
      "Predictions stored: 580 files\n",
      "Failed: 0 images\n",
      "Total processed: 580 images\n",
      "======================================================================\n",
      "======================================================================\n",
      "✓ 590 images processed\n",
      "Predictions stored: 590 files\n",
      "Failed: 0 images\n",
      "Total processed: 590 images\n",
      "======================================================================\n",
      "======================================================================\n",
      "✓ 600 images processed\n",
      "Predictions stored: 600 files\n",
      "Failed: 0 images\n",
      "Total processed: 600 images\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "PROCESSING COMPLETE\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Process all test images with hybrid approach\n",
    "all_test_files = sorted([f for f in os.listdir(TEST_INPUT_LABELS) if f.endswith('.json')])\n",
    "print(f\"Processing {len(all_test_files)} test images...\\n\")\n",
    "\n",
    "detailed_results = []\n",
    "all_predictions = {}\n",
    "failed_count = 0\n",
    "processed_count = 0\n",
    "\n",
    "def match_error_bars_to_points(data_points: List[Dict], detected_bars: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Match detected error bars to known data points.\n",
    "    For each data point, find the nearest error bar.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for point in data_points:\n",
    "        x, y = point['x'], point['y']\n",
    "        \n",
    "        # Find nearest error bar (by x coordinate)\n",
    "        best_bar = None\n",
    "        min_dist = float('inf')\n",
    "        \n",
    "        for bar in detected_bars:\n",
    "            x_dist = abs(bar['x'] - x)\n",
    "            if x_dist < min_dist:\n",
    "                min_dist = x_dist\n",
    "                best_bar = bar\n",
    "        \n",
    "        # If found a nearby bar (within 10px), use it\n",
    "        if best_bar and min_dist < 10:\n",
    "            top_dist = abs(y - best_bar['top'])\n",
    "            bottom_dist = abs(best_bar['bottom'] - y)\n",
    "            \n",
    "            results.append({\n",
    "                \"x\": x,\n",
    "                \"y\": y,\n",
    "                \"label\": \"\",\n",
    "                \"topBarPixelDistance\": float(top_dist),\n",
    "                \"bottomBarPixelDistance\": float(bottom_dist),\n",
    "                \"deviationPixelDistance\": float(max(top_dist, bottom_dist))\n",
    "            })\n",
    "        else:\n",
    "            # No error bar detected at this point\n",
    "            results.append({\n",
    "                \"x\": x,\n",
    "                \"y\": y,\n",
    "                \"label\": \"\",\n",
    "                \"topBarPixelDistance\": 0.0,\n",
    "                \"bottomBarPixelDistance\": 0.0,\n",
    "                \"deviationPixelDistance\": 0.0\n",
    "            })\n",
    "    \n",
    "    return results\n",
    "\n",
    "for i, json_file in enumerate(all_test_files):\n",
    "    try:\n",
    "        # Load input\n",
    "        input_json = load_test_input(os.path.join(TEST_INPUT_LABELS, json_file))\n",
    "        image_file = input_json['image_file']\n",
    "        image_path = os.path.join(TEST_IMAGES, image_file)\n",
    "        \n",
    "        # Analyze image once\n",
    "        analysis = analyze_plot(image_path)\n",
    "        \n",
    "        if not analysis or not analysis.get('pixel_measurements'):\n",
    "            failed_count += 1\n",
    "            continue\n",
    "        \n",
    "        detected_bars = analysis['pixel_measurements']\n",
    "        \n",
    "        # Process each line in the data\n",
    "        predictions = []\n",
    "        \n",
    "        for data_line in input_json.get('data_points', []):\n",
    "            line_name = data_line.get('lineName', '')\n",
    "            points = data_line.get('points', [])\n",
    "            \n",
    "            # Match detected bars to data points\n",
    "            matched_points = match_error_bars_to_points(points, detected_bars)\n",
    "            \n",
    "            predictions.append({\n",
    "                'label': {'lineName': line_name},\n",
    "                'points': matched_points\n",
    "            })\n",
    "        \n",
    "        if not predictions:\n",
    "            failed_count += 1\n",
    "            continue\n",
    "        \n",
    "        # Store predictions\n",
    "        all_predictions[json_file] = {\n",
    "            'image_file': image_file,\n",
    "            'predictions': predictions,\n",
    "            'input_json': input_json\n",
    "        }\n",
    "        \n",
    "        # Evaluate\n",
    "        gt_json = load_ground_truth(os.path.join(TEST_GROUND_TRUTH, json_file))\n",
    "        \n",
    "        image_errors = []\n",
    "        for pred_line in predictions:\n",
    "            gt_line = None\n",
    "            for gt in gt_json:\n",
    "                if gt.get('label', {}).get('lineName') == pred_line.get('label', {}).get('lineName'):\n",
    "                    gt_line = gt\n",
    "                    break\n",
    "            \n",
    "            if gt_line is None:\n",
    "                continue\n",
    "            \n",
    "            pred_points = [p for p in pred_line.get('points', []) \n",
    "                          if p.get('label', '') not in ['xmin', 'xmax', 'ymin', 'ymax']]\n",
    "            gt_points = [p for p in gt_line.get('points', []) \n",
    "                        if p.get('label', '') not in ['xmin', 'xmax', 'ymin', 'ymax']]\n",
    "            \n",
    "            for pred_pt, gt_pt in zip(pred_points, gt_points):\n",
    "                error = calculate_point_error(pred_pt, gt_pt)\n",
    "                image_errors.append(error)\n",
    "        \n",
    "        if image_errors:\n",
    "            img_metrics = {\n",
    "                'image_file': image_file,\n",
    "                'json_file': json_file,\n",
    "                'num_points': len(image_errors),\n",
    "                'mean_top_error': np.mean([e['top_error'] for e in image_errors]),\n",
    "                'mean_bottom_error': np.mean([e['bottom_error'] for e in image_errors]),\n",
    "                'mean_deviation_error': np.mean([e['deviation_error'] for e in image_errors]),\n",
    "                'mean_overall_error': np.mean([e['mean_error'] for e in image_errors]),\n",
    "                'max_top_error': np.max([e['top_error'] for e in image_errors]),\n",
    "                'max_bottom_error': np.max([e['bottom_error'] for e in image_errors]),\n",
    "                'all_errors': image_errors\n",
    "            }\n",
    "            detailed_results.append(img_metrics)\n",
    "        \n",
    "        processed_count += 1\n",
    "        \n",
    "        # Progress update every 10 images\n",
    "        if processed_count % 10 == 0:\n",
    "            print(f\"{'='*70}\")\n",
    "            print(f\"✓ {processed_count} images processed\")\n",
    "            print(f\"Predictions stored: {len(all_predictions)} files\")\n",
    "            print(f\"Failed: {failed_count} images\")\n",
    "            print(f\"Total processed: {processed_count} images\")\n",
    "            print(f\"{'='*70}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        failed_count += 1\n",
    "        if failed_count <= 5:\n",
    "            print(f\"✗ Error processing {json_file}: {e}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"PROCESSING COMPLETE\")\n",
    "print(f\"{'='*70}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4855ec92",
   "metadata": {},
   "source": [
    "## 11. Comprehensive Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "671b7c6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T13:41:22.327554Z",
     "iopub.status.busy": "2026-01-28T13:41:22.327222Z",
     "iopub.status.idle": "2026-01-28T13:41:22.394379Z",
     "shell.execute_reply": "2026-01-28T13:41:22.393751Z",
     "shell.execute_reply.started": "2026-01-28T13:41:22.327529Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "VQA + COT ERROR BAR DETECTION - EVALUATION RESULTS\n",
      "======================================================================\n",
      "\n",
      "Dataset Statistics:\n",
      "----------------------------------------------------------------------\n",
      "  Total Test Images........................................... 600\n",
      "  Total Data Points........................................... 10229\n",
      "  Average Points per Image.................................... 17.05\n",
      "\n",
      "Absolute Pixel Error - Top Error Bar:\n",
      "----------------------------------------------------------------------\n",
      "  Mean........................................................ 42.40\n",
      "  Median...................................................... 20.05\n",
      "  Std Dev..................................................... 69.38\n",
      "  Min......................................................... 0.00\n",
      "  Max......................................................... 1632.75\n",
      "  25th Percentile............................................. 6.62\n",
      "  75th Percentile............................................. 45.13\n",
      "\n",
      "Absolute Pixel Error - Bottom Error Bar:\n",
      "----------------------------------------------------------------------\n",
      "  Mean........................................................ 38.04\n",
      "  Median...................................................... 18.81\n",
      "  Std Dev..................................................... 60.21\n",
      "  Min......................................................... 0.00\n",
      "  Max......................................................... 1300.38\n",
      "  25th Percentile............................................. 5.44\n",
      "  75th Percentile............................................. 42.85\n",
      "\n",
      "Overall Mean Pixel Error:\n",
      "----------------------------------------------------------------------\n",
      "  Mean........................................................ 40.22\n",
      "  Median...................................................... 24.18\n",
      "  Std Dev..................................................... 54.55\n",
      "  RMSE........................................................ 67.77\n",
      "\n",
      "Accuracy Metrics (% within threshold):\n",
      "----------------------------------------------------------------------\n",
      "  Within 5 pixels............................................. 17.50\n",
      "  Within 10 pixels............................................ 27.18\n",
      "  Within 20 pixels............................................ 44.09\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Saved: /kaggle/working/qwen_vqa_per_image_metrics.csv\n",
      "Saved: /kaggle/working/qwen_vqa_summary_metrics.csv\n"
     ]
    }
   ],
   "source": [
    "if detailed_results:\n",
    "    # Collect all individual point errors\n",
    "    all_point_errors = []\n",
    "    for img_result in detailed_results:\n",
    "        all_point_errors.extend(img_result['all_errors'])\n",
    "    \n",
    "    total_images = len(detailed_results)\n",
    "    total_points = sum(img['num_points'] for img in detailed_results)\n",
    "    \n",
    "    # Absolute pixel errors\n",
    "    all_top_errors = [e['top_error'] for e in all_point_errors]\n",
    "    all_bottom_errors = [e['bottom_error'] for e in all_point_errors]\n",
    "    all_deviation_errors = [e['deviation_error'] for e in all_point_errors]\n",
    "    all_mean_errors = [e['mean_error'] for e in all_point_errors]\n",
    "    \n",
    "    # Calculate accuracy metrics\n",
    "    threshold_5px = sum(1 for e in all_mean_errors if e <= 5) / len(all_mean_errors) * 100\n",
    "    threshold_10px = sum(1 for e in all_mean_errors if e <= 10) / len(all_mean_errors) * 100\n",
    "    threshold_20px = sum(1 for e in all_mean_errors if e <= 20) / len(all_mean_errors) * 100\n",
    "    \n",
    "    paper_metrics = {\n",
    "        'Dataset Statistics': {\n",
    "            'Total Test Images': total_images,\n",
    "            'Total Data Points': total_points,\n",
    "            'Average Points per Image': total_points / total_images,\n",
    "        },\n",
    "        'Absolute Pixel Error - Top Error Bar': {\n",
    "            'Mean': np.mean(all_top_errors),\n",
    "            'Median': np.median(all_top_errors),\n",
    "            'Std Dev': np.std(all_top_errors),\n",
    "            'Min': np.min(all_top_errors),\n",
    "            'Max': np.max(all_top_errors),\n",
    "            '25th Percentile': np.percentile(all_top_errors, 25),\n",
    "            '75th Percentile': np.percentile(all_top_errors, 75),\n",
    "        },\n",
    "        'Absolute Pixel Error - Bottom Error Bar': {\n",
    "            'Mean': np.mean(all_bottom_errors),\n",
    "            'Median': np.median(all_bottom_errors),\n",
    "            'Std Dev': np.std(all_bottom_errors),\n",
    "            'Min': np.min(all_bottom_errors),\n",
    "            'Max': np.max(all_bottom_errors),\n",
    "            '25th Percentile': np.percentile(all_bottom_errors, 25),\n",
    "            '75th Percentile': np.percentile(all_bottom_errors, 75),\n",
    "        },\n",
    "        'Overall Mean Pixel Error': {\n",
    "            'Mean': np.mean(all_mean_errors),\n",
    "            'Median': np.median(all_mean_errors),\n",
    "            'Std Dev': np.std(all_mean_errors),\n",
    "            'RMSE': np.sqrt(np.mean(np.array(all_mean_errors)**2)),\n",
    "        },\n",
    "        'Accuracy Metrics (% within threshold)': {\n",
    "            'Within 5 pixels': threshold_5px,\n",
    "            'Within 10 pixels': threshold_10px,\n",
    "            'Within 20 pixels': threshold_20px,\n",
    "        },\n",
    "    }\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"VQA + COT ERROR BAR DETECTION - EVALUATION RESULTS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    for section, metrics in paper_metrics.items():\n",
    "        print(f\"\\n{section}:\")\n",
    "        print(\"-\" * 70)\n",
    "        for metric, value in metrics.items():\n",
    "            if isinstance(value, float):\n",
    "                print(f\"  {metric:.<60} {value:.2f}\")\n",
    "            else:\n",
    "                print(f\"  {metric:.<60} {value}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    \n",
    "    # Save metrics\n",
    "    import pandas as pd\n",
    "    \n",
    "    per_image_df = pd.DataFrame([{\n",
    "        'Image': img['image_file'],\n",
    "        'Points': img['num_points'],\n",
    "        'Mean_Top_Error': img['mean_top_error'],\n",
    "        'Mean_Bottom_Error': img['mean_bottom_error'],\n",
    "        'Mean_Overall_Error': img['mean_overall_error'],\n",
    "        'Max_Top_Error': img['max_top_error'],\n",
    "        'Max_Bottom_Error': img['max_bottom_error'],\n",
    "    } for img in detailed_results])\n",
    "    \n",
    "    per_image_df.to_csv('/kaggle/working/qwen_vqa_per_image_metrics.csv', index=False)\n",
    "    print(\"\\nSaved: /kaggle/working/qwen_vqa_per_image_metrics.csv\")\n",
    "    \n",
    "    summary_df = pd.DataFrame([\n",
    "        {'Metric': 'Method', 'Value': 'Qwen2.5-VL VQA+CoT 4-bit'},\n",
    "        {'Metric': 'Total Images', 'Value': total_images},\n",
    "        {'Metric': 'Total Points', 'Value': total_points},\n",
    "        {'Metric': 'Mean Top Error (px)', 'Value': f\"{np.mean(all_top_errors):.2f}\"},\n",
    "        {'Metric': 'Mean Bottom Error (px)', 'Value': f\"{np.mean(all_bottom_errors):.2f}\"},\n",
    "        {'Metric': 'Mean Overall Error (px)', 'Value': f\"{np.mean(all_mean_errors):.2f}\"},\n",
    "        {'Metric': 'RMSE (px)', 'Value': f\"{np.sqrt(np.mean(np.array(all_mean_errors)**2)):.2f}\"},\n",
    "        {'Metric': 'Accuracy @ 5px (%)', 'Value': f\"{threshold_5px:.2f}\"},\n",
    "        {'Metric': 'Accuracy @ 10px (%)', 'Value': f\"{threshold_10px:.2f}\"},\n",
    "        {'Metric': 'Accuracy @ 20px (%)', 'Value': f\"{threshold_20px:.2f}\"},\n",
    "    ])\n",
    "    \n",
    "    summary_df.to_csv('/kaggle/working/qwen_vqa_summary_metrics.csv', index=False)\n",
    "    print(\"Saved: /kaggle/working/qwen_vqa_summary_metrics.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e570d9e3",
   "metadata": {},
   "source": [
    "## 12. Save Predictions in Required Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a8f4d1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T13:41:22.395845Z",
     "iopub.status.busy": "2026-01-28T13:41:22.395320Z",
     "iopub.status.idle": "2026-01-28T13:41:22.693043Z",
     "shell.execute_reply": "2026-01-28T13:41:22.692475Z",
     "shell.execute_reply.started": "2026-01-28T13:41:22.395821Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving 600 predictions...\n",
      "\n",
      "Saved 100/600 files...\n",
      "Saved 200/600 files...\n",
      "Saved 300/600 files...\n",
      "Saved 400/600 files...\n",
      "Saved 500/600 files...\n",
      "Saved 600/600 files...\n",
      "\n",
      "Successfully saved 600 prediction files\n",
      "\n",
      "======================================================================\n",
      "SAMPLE OUTPUT FORMAT\n",
      "======================================================================\n",
      "{\n",
      "  \"data_point\": {\n",
      "    \"x\": 96.58120273413908,\n",
      "    \"y\": 70.94969906573277\n",
      "  },\n",
      "  \"upper_error_bar\": {\n",
      "    \"x\": 96.58120273413908,\n",
      "    \"y\": 10.0\n",
      "  },\n",
      "  \"lower_error_bar\": {\n",
      "    \"x\": 96.58120273413908,\n",
      "    \"y\": 204.0\n",
      "  }\n",
      "}\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Save all predictions\n",
    "OUTPUT_DIR = \"/kaggle/working/vlm_predictions\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Saving {len(all_predictions)} predictions...\\n\")\n",
    "\n",
    "saved_count = 0\n",
    "for json_file, pred_data in all_predictions.items():\n",
    "    try:\n",
    "        image_file = pred_data['image_file']\n",
    "        predictions = pred_data['predictions']\n",
    "        \n",
    "        # Convert to output format\n",
    "        output_json = convert_to_output_format(image_file, predictions)\n",
    "        \n",
    "        output_path = os.path.join(OUTPUT_DIR, json_file)\n",
    "        with open(output_path, 'w') as f:\n",
    "            json.dump(output_json, f, indent=2)\n",
    "        \n",
    "        saved_count += 1\n",
    "        \n",
    "        if saved_count % 100 == 0:\n",
    "            print(f\"Saved {saved_count}/{len(all_predictions)} files...\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving {json_file}: {e}\")\n",
    "\n",
    "print(f\"\\nSuccessfully saved {saved_count} prediction files\")\n",
    "\n",
    "# Show sample output format\n",
    "if all_predictions:\n",
    "    sample_file = list(all_predictions.keys())[0]\n",
    "    sample_pred = all_predictions[sample_file]\n",
    "    sample_output = convert_to_output_format(\n",
    "        sample_pred['image_file'], \n",
    "        sample_pred['predictions']\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"SAMPLE OUTPUT FORMAT\")\n",
    "    print(\"=\"*70)\n",
    "    if sample_output['error_bars'] and sample_output['error_bars'][0]['points']:\n",
    "        print(json.dumps(sample_output['error_bars'][0]['points'][0], indent=2))\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5734238f",
   "metadata": {},
   "source": [
    "## 13. Create ZIP Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d829524e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T13:41:22.694114Z",
     "iopub.status.busy": "2026-01-28T13:41:22.693873Z",
     "iopub.status.idle": "2026-01-28T13:41:22.810892Z",
     "shell.execute_reply": "2026-01-28T13:41:22.810259Z",
     "shell.execute_reply.started": "2026-01-28T13:41:22.694092Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating ZIP archive: /kaggle/working/qwen_vqa_error_bar_predictions_20260128_134122.zip\n",
      "\n",
      "======================================================================\n",
      "QWEN VQA+CoT PREDICTIONS - ZIP ARCHIVE CREATED\n",
      "======================================================================\n",
      "Filename: /kaggle/working/qwen_vqa_error_bar_predictions_20260128_134122.zip\n",
      "Size: 0.63 MB\n",
      "Contents:\n",
      "  - 600 prediction JSON files\n",
      "  - qwen_vqa_per_image_metrics.csv\n",
      "  - qwen_vqa_summary_metrics.csv\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "from datetime import datetime\n",
    "\n",
    "zip_filename = f\"/kaggle/working/qwen_vqa_error_bar_predictions_{datetime.now().strftime('%Y%m%d_%H%M%S')}.zip\"\n",
    "\n",
    "print(f\"Creating ZIP archive: {zip_filename}\\n\")\n",
    "\n",
    "with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    # Add predictions\n",
    "    for json_file in os.listdir(OUTPUT_DIR):\n",
    "        if json_file.endswith('.json'):\n",
    "            file_path = os.path.join(OUTPUT_DIR, json_file)\n",
    "            zipf.write(file_path, arcname=f\"predictions/{json_file}\")\n",
    "    \n",
    "    # Add metrics\n",
    "    if os.path.exists('/kaggle/working/qwen_vqa_per_image_metrics.csv'):\n",
    "        zipf.write('/kaggle/working/qwen_vqa_per_image_metrics.csv', arcname='qwen_vqa_per_image_metrics.csv')\n",
    "    if os.path.exists('/kaggle/working/qwen_vqa_summary_metrics.csv'):\n",
    "        zipf.write('/kaggle/working/qwen_vqa_summary_metrics.csv', arcname='qwen_vqa_summary_metrics.csv')\n",
    "\n",
    "zip_size_mb = os.path.getsize(zip_filename) / (1024 * 1024)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"QWEN VQA+CoT PREDICTIONS - ZIP ARCHIVE CREATED\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Filename: {zip_filename}\")\n",
    "print(f\"Size: {zip_size_mb:.2f} MB\")\n",
    "print(f\"Contents:\")\n",
    "print(f\"  - {saved_count} prediction JSON files\")\n",
    "print(f\"  - qwen_vqa_per_image_metrics.csv\")\n",
    "print(f\"  - qwen_vqa_summary_metrics.csv\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731abaa3",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook implements **VQA + Chain-of-Thought (CoT) error bar detection** using Qwen2.5-VL-7B-Instruct with:\n",
    "\n",
    "### Approach:\n",
    "1. **4-bit Quantized Qwen2.5-VL Model**: Memory-efficient model loading\n",
    "2. **Step-by-Step CoT Reasoning**: 5-step process for each data point:\n",
    "   - Step 1: Locate and describe the marker\n",
    "   - Step 2: Detect error bar presence\n",
    "   - Step 3: Find top endpoint with reasoning\n",
    "   - Step 4: Find bottom endpoint with reasoning\n",
    "   - Step 5: Verify measurements for consistency\n",
    "3. **Deterministic Inference**: Low temperature, no sampling for consistency\n",
    "4. **Reasoning Trace + Measurements**: Extract both CoT steps and final pixel coordinates\n",
    "\n",
    "### Key Features:\n",
    "- Qwen2.5-VL-7B-Instruct with 4-bit quantization\n",
    "- Comprehensive evaluation metrics (pixel errors, RMSE, accuracy@threshold)\n",
    "- ZIP archive with all predictions\n",
    "- Explainable reasoning trace for each measurement\n",
    "- JSON output with both predictions and reasoning\n",
    "- Optimized for fast inference (768px images, 1024 max tokens)\n",
    "\n",
    "### Model Configuration:\n",
    "- **Model**: Qwen2-VL-7B-Instruct\n",
    "- **Quantization**: 4-bit NF4 with double quantization\n",
    "- **Temperature**: 0.1 (deterministic)\n",
    "- **Max Tokens**: 4096 (for CoT reasoning)\n",
    "- **Device**: Auto-mapped to available GPUs\n",
    "\n",
    "### Advantages:\n",
    "- **Explicit Reasoning**: Shows step-by-step thought process\n",
    "- **Self-Verification**: Built-in consistency checks\n",
    "- **Better Accuracy**: CoT forces careful visual examination\n",
    "- **Explainable**: Can debug by examining reasoning trace\n",
    "- **Adaptive**: Handles diverse plot styles without parameter tuning\n",
    "- **Contextual**: Understands chart structure and conventions\n",
    "- **Transparency**: Shows how the model arrives at measurements"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 9353118,
     "sourceId": 14642164,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31260,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
