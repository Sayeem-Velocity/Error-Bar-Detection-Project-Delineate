{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14642164,"sourceType":"datasetVersion","datasetId":9353118}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"06d3bb23","cell_type":"markdown","source":"# Qwen2.5-VL Error Bar Detection Fine-tuning\n\nThis notebook fine-tunes Qwen2.5-VL-7B-Instruct for error bar detection in scientific plots.\n\n## Task:\n- **Input**: Scientific plot image + data point coordinates (x, y)\n- **Output**: Error bar distances (topBarPixelDistance, bottomBarPixelDistance)\n\n## Approach:\n- Use Qwen2.5-VL (Vision-Language Model) with LoRA fine-tuning\n- Train on labeled data with known error bar distances\n- Evaluate on test data","metadata":{}},{"id":"bf695361","cell_type":"markdown","source":"## 1. Install Required Libraries","metadata":{}},{"id":"0da9d222","cell_type":"code","source":"# Install libraries for VLM fine-tuning\n!pip install -U transformers accelerate bitsandbytes -q\n!pip install -U peft trl datasets -q\n!pip install -U huggingface_hub -q\n!pip install pillow pandas tqdm -q\n!pip install qwen-vl-utils -q  # For Qwen VL processing\n\nprint(\"All libraries installed successfully!\")","metadata":{"execution":{"iopub.status.busy":"2026-01-29T11:40:00.843764Z","iopub.execute_input":"2026-01-29T11:40:00.844084Z","iopub.status.idle":"2026-01-29T11:40:37.282584Z","shell.execute_reply.started":"2026-01-29T11:40:00.844058Z","shell.execute_reply":"2026-01-29T11:40:37.281720Z"},"trusted":true},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m79.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m380.9/380.9 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.7/536.7 kB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nsentence-transformers 5.1.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 5.0.0 which is incompatible.\ngradio 5.49.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.5 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.0/557.0 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m532.9/532.9 kB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m515.2/515.2 kB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 MB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hAll libraries installed successfully!\n","output_type":"stream"}],"execution_count":1},{"id":"71ca4f61","cell_type":"markdown","source":"## 2. Import Libraries","metadata":{}},{"id":"e1d39ffc","cell_type":"code","source":"import torch\nimport os\nimport json\nimport gc\nimport numpy as np\nfrom pathlib import Path\nfrom typing import List, Dict, Optional, Tuple\nfrom PIL import Image\nfrom tqdm import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Transformers\nfrom transformers import (\n    Qwen2_5_VLForConditionalGeneration,\n    AutoProcessor,\n    BitsAndBytesConfig,\n    TrainingArguments,\n)\n\n# PEFT for LoRA\nfrom peft import (\n    LoraConfig,\n    get_peft_model,\n    prepare_model_for_kbit_training,\n    TaskType,\n    PeftModel,\n)\n\n# Datasets\nfrom datasets import Dataset\n\n# HuggingFace Hub\nfrom huggingface_hub import login, HfApi, create_repo\n\n# Check GPU\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")","metadata":{"execution":{"iopub.status.busy":"2026-01-29T11:40:37.284400Z","iopub.execute_input":"2026-01-29T11:40:37.284697Z","iopub.status.idle":"2026-01-29T11:40:59.100890Z","shell.execute_reply.started":"2026-01-29T11:40:37.284665Z","shell.execute_reply":"2026-01-29T11:40:59.100112Z"},"trusted":true},"outputs":[{"name":"stdout","text":"PyTorch version: 2.8.0+cu126\nCUDA available: True\nGPU: Tesla T4\nGPU Memory: 15.8 GB\n","output_type":"stream"}],"execution_count":2},{"id":"e0c0dc3f","cell_type":"markdown","source":"## 3. Configuration","metadata":{}},{"id":"a8b100b4","cell_type":"code","source":"import os\n\n# ============== DATA PATHS (Kaggle) ==============\nBASE_PATH = \"/kaggle/input/graph-plots\"\n\n# Training data\nTRAIN_IMAGES = os.path.join(BASE_PATH, \"Train\", \"images\")\nTRAIN_LABELS = os.path.join(BASE_PATH, \"Train\", \"labels\")  # Ground truth with error bars\n\n# Test data\nTEST_IMAGES = os.path.join(BASE_PATH, \"Test\", \"images\")\nTEST_INPUT_LABELS = os.path.join(BASE_PATH, \"Test\", \"test_labels\")  # Input: x,y only\nTEST_GROUND_TRUTH = os.path.join(BASE_PATH, \"Test\", \"labels\")  # Ground truth with error bars\n\n# ============== MODEL CONFIGURATION ==============\nBASE_MODEL = \"Qwen/Qwen2.5-VL-7B-Instruct\"\nNEW_MODEL_NAME = \"Chartqwen\"\n\n# HuggingFace Hub\nHF_USERNAME = \"Sayeem26s\"\nHF_REPO_NAME = \"Chartqwen\"\nHF_REPO_ID = f\"{HF_USERNAME}/{HF_REPO_NAME}\"\n\n# ============== TRAINING CONFIGURATION ==============\nOUTPUT_DIR = \"/kaggle/working/qwen_vl_lora_output\"\nCHECKPOINT_DIR = \"/kaggle/working/qwen_vl_checkpoints\"\n\nMAX_SEQ_LENGTH = 2048\nIMAGE_MAX_SIZE = 768\n\nBATCH_SIZE = 1  # VLM requires small batch size\nGRADIENT_ACCUMULATION_STEPS = 16  # Effective batch = 16\n\nLEARNING_RATE = 2e-4  # Slightly higher for faster convergence\nNUM_EPOCHS = 1  # 1 epoch for 600 samples\nWARMUP_RATIO = 0.03\n\nSAVE_STEPS = 30  # Save every 30 steps\nLOGGING_STEPS = 5  # Log frequently\n\nMAX_TRAIN_SAMPLES = 600  # Fast training: ~1-1.5 hours\nMAX_GRAD_NORM = 1.0\n\n# ============== LoRA CONFIGURATION ==============\nLORA_R = 32\nLORA_ALPHA = 64\nLORA_DROPOUT = 0.05\n\n# ============== CREATE DIRECTORIES ==============\nos.makedirs(OUTPUT_DIR, exist_ok=True)\nos.makedirs(CHECKPOINT_DIR, exist_ok=True)\n\n# ============== PRINT CONFIGURATION ==============\nprint(\"Configuration:\")\nprint(f\"  Base Model          : {BASE_MODEL}\")\nprint(f\"  New Model           : {NEW_MODEL_NAME}\")\nprint(f\"  Train Images        : {TRAIN_IMAGES}\")\nprint(f\"  Train Labels        : {TRAIN_LABELS}\")\nprint(f\"  Test Images         : {TEST_IMAGES}\")\nprint(f\"  HF Repo             : {HF_REPO_ID}\")\n\nprint(\"\\nTraining Settings:\")\nprint(f\"  - Samples           : {MAX_TRAIN_SAMPLES}\")\nprint(f\"  - Epochs            : {NUM_EPOCHS}\")\nprint(f\"  - Effective Batch   : {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")\nprint(f\"  - Learning Rate     : {LEARNING_RATE}\")\nprint(f\"  - Est. Time         : ~1 hour on T4 GPU\")\n\nprint(\"\\nLoRA Settings:\")\nprint(f\"  - Rank: {LORA_R}, Alpha: {LORA_ALPHA}, Dropout: {LORA_DROPOUT}\")","metadata":{"execution":{"iopub.status.busy":"2026-01-29T11:40:59.101784Z","iopub.execute_input":"2026-01-29T11:40:59.102382Z","iopub.status.idle":"2026-01-29T11:40:59.113182Z","shell.execute_reply.started":"2026-01-29T11:40:59.102354Z","shell.execute_reply":"2026-01-29T11:40:59.112163Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Configuration:\n  Base Model          : Qwen/Qwen2.5-VL-7B-Instruct\n  New Model           : Chartqwen\n  Train Images        : /kaggle/input/graph-plots/Train/images\n  Train Labels        : /kaggle/input/graph-plots/Train/labels\n  Test Images         : /kaggle/input/graph-plots/Test/images\n  HF Repo             : Sayeem26s/Chartqwen\n\nTraining Settings:\n  - Samples           : 600\n  - Epochs            : 1\n  - Effective Batch   : 16\n  - Learning Rate     : 0.0002\n  - Est. Time         : ~1 hour on T4 GPU\n\nLoRA Settings:\n  - Rank: 32, Alpha: 64, Dropout: 0.05\n","output_type":"stream"}],"execution_count":3},{"id":"ffbfdb0d","cell_type":"markdown","source":"## 4. Login to HuggingFace Hub","metadata":{}},{"id":"ee8226e7","cell_type":"code","source":"# Login to HuggingFace Hub\ntry:\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n    HF_TOKEN = user_secrets.get_secret(\"HF_TOKEN\")\n    print(\"HF Token loaded from Kaggle Secrets\")\nexcept:\n    HF_TOKEN = \"YOUR_HF_TOKEN_HERE\"  # Replace with your token\n    print(\"Using hardcoded HF token (replace with your actual token)\")\n\nlogin(token=HF_TOKEN)\nprint(f\"Logged in to HuggingFace Hub as: {HF_USERNAME}\")","metadata":{"execution":{"iopub.status.busy":"2026-01-29T11:40:59.114246Z","iopub.execute_input":"2026-01-29T11:40:59.114812Z","iopub.status.idle":"2026-01-29T11:40:59.408816Z","shell.execute_reply.started":"2026-01-29T11:40:59.114775Z","shell.execute_reply":"2026-01-29T11:40:59.408091Z"},"trusted":true},"outputs":[{"name":"stdout","text":"HF Token loaded from Kaggle Secrets\nLogged in to HuggingFace Hub as: Sayeem26s\n","output_type":"stream"}],"execution_count":4},{"id":"70c3a335","cell_type":"markdown","source":"## 5. Load and Prepare Training Data","metadata":{}},{"id":"15745da5","cell_type":"code","source":"def load_label_file(json_path: str) -> List[Dict]:\n    \"\"\"Load label JSON file.\"\"\"\n    with open(json_path, 'r') as f:\n        return json.load(f)\n\ndef get_image_filename(label_filename: str, images_dir: str) -> Optional[str]:\n    \"\"\"\n    Find corresponding image file for a label.\n    Label: xxxx.json -> Image: xxxx.png or xxxx.jpg\n    \"\"\"\n    base_name = label_filename.replace('.json', '')\n    \n    for ext in ['.png', '.jpg', '.jpeg', '.PNG', '.JPG', '.JPEG']:\n        img_path = os.path.join(images_dir, base_name + ext)\n        if os.path.exists(img_path):\n            return img_path\n    return None\n\ndef create_training_sample(image_path: str, label_data: List[Dict]) -> Dict:\n    \"\"\"\n    Create a training sample from image and labels.\n    \n    Input format (label_data):\n    [\n        {\n            \"label\": {\"lineName\": \"...\"},\n            \"points\": [\n                {\"x\": 100, \"y\": 200, \"topBarPixelDistance\": 10, \"bottomBarPixelDistance\": 15, ...},\n                ...\n            ]\n        }\n    ]\n    \n    Output format:\n    {\n        \"image_path\": \"path/to/image.png\",\n        \"input_points\": [{\"x\": 100, \"y\": 200}, ...],\n        \"output_points\": [{\"x\": 100, \"y\": 200, \"topBarPixelDistance\": 10, \"bottomBarPixelDistance\": 15}, ...]\n    }\n    \"\"\"\n    all_input_points = []\n    all_output_points = []\n    \n    for line_data in label_data:\n        points = line_data.get('points', [])\n        for pt in points:\n            # Skip axis labels\n            if pt.get('label', '') in ['xmin', 'xmax', 'ymin', 'ymax']:\n                continue\n            \n            x = pt.get('x', 0)\n            y = pt.get('y', 0)\n            top_dist = pt.get('topBarPixelDistance', 0)\n            bottom_dist = pt.get('bottomBarPixelDistance', 0)\n            \n            all_input_points.append({\"x\": round(x, 1), \"y\": round(y, 1)})\n            all_output_points.append({\n                \"x\": round(x, 1),\n                \"y\": round(y, 1),\n                \"topBarPixelDistance\": round(top_dist, 1),\n                \"bottomBarPixelDistance\": round(bottom_dist, 1)\n            })\n    \n    return {\n        \"image_path\": image_path,\n        \"input_points\": all_input_points,\n        \"output_points\": all_output_points\n    }\n\ndef load_all_training_data(labels_dir: str, images_dir: str, max_samples: Optional[int] = None) -> List[Dict]:\n    \"\"\"\n    Load all training samples from labels directory.\n    \"\"\"\n    samples = []\n    label_files = sorted([f for f in os.listdir(labels_dir) if f.endswith('.json')])\n    \n    if max_samples:\n        label_files = label_files[:max_samples]\n    \n    print(f\"Loading {len(label_files)} training samples...\")\n    \n    for label_file in tqdm(label_files):\n        try:\n            # Find image\n            image_path = get_image_filename(label_file, images_dir)\n            if not image_path:\n                continue\n            \n            # Load labels\n            label_path = os.path.join(labels_dir, label_file)\n            label_data = load_label_file(label_path)\n            \n            # Create training sample\n            sample = create_training_sample(image_path, label_data)\n            \n            # Only include samples with points\n            if sample['input_points']:\n                samples.append(sample)\n                \n        except Exception as e:\n            print(f\"Error loading {label_file}: {e}\")\n            continue\n    \n    print(f\"Loaded {len(samples)} valid training samples\")\n    return samples\n\n# Load training data\nprint(\"Loading training data...\")\ntraining_samples = load_all_training_data(TRAIN_LABELS, TRAIN_IMAGES, MAX_TRAIN_SAMPLES)\n\n# Show sample\nif training_samples:\n    print(f\"\\nSample training data:\")\n    print(f\"  Image: {training_samples[0]['image_path']}\")\n    print(f\"  Num points: {len(training_samples[0]['input_points'])}\")\n    if training_samples[0]['input_points']:\n        print(f\"  First input: {training_samples[0]['input_points'][0]}\")\n        print(f\"  First output: {training_samples[0]['output_points'][0]}\")","metadata":{"execution":{"iopub.status.busy":"2026-01-29T11:40:59.410543Z","iopub.execute_input":"2026-01-29T11:40:59.410894Z","iopub.status.idle":"2026-01-29T11:41:18.938696Z","shell.execute_reply.started":"2026-01-29T11:40:59.410869Z","shell.execute_reply":"2026-01-29T11:41:18.937925Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Loading training data...\nLoading 600 training samples...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 600/600 [00:18<00:00, 31.71it/s]","output_type":"stream"},{"name":"stdout","text":"Loaded 600 valid training samples\n\nSample training data:\n  Image: /kaggle/input/graph-plots/Train/images/001f3fd0-23eb-4371-a52a-41b9b71c36bf.png\n  Num points: 23\n  First input: {'x': 177.0, 'y': 94.2}\n  First output: {'x': 177.0, 'y': 94.2, 'topBarPixelDistance': 100.7, 'bottomBarPixelDistance': 52.7}\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":5},{"id":"6c7c171c","cell_type":"markdown","source":"## 6. Load Model and Processor","metadata":{}},{"id":"343a92b7","cell_type":"code","source":"# 4-bit Quantization config\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16,\n    bnb_4bit_use_double_quant=True,\n)\n\nprint(\"Loading processor...\")\nprocessor = AutoProcessor.from_pretrained(\n    BASE_MODEL,\n    trust_remote_code=True,\n)\n\n# Set pad token\nif processor.tokenizer.pad_token is None:\n    processor.tokenizer.pad_token = processor.tokenizer.eos_token\n\nprint(f\"Processor loaded!\")\n\nprint(\"\\nLoading model with 4-bit quantization...\")\nprint(\"This may take 3-5 minutes...\")\n\nmodel = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n    BASE_MODEL,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    trust_remote_code=True,\n    torch_dtype=torch.bfloat16,\n)\n\nprint(f\"Model loaded!\")\n\n# Prepare model for k-bit training\nmodel = prepare_model_for_kbit_training(model)\n\n# Disable cache for training (required for gradient checkpointing)\nmodel.config.use_cache = False\n\nif torch.cuda.is_available():\n    print(f\"GPU Memory Used: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")","metadata":{"execution":{"iopub.status.busy":"2026-01-29T11:41:18.939570Z","iopub.execute_input":"2026-01-29T11:41:18.939834Z","iopub.status.idle":"2026-01-29T11:44:17.737485Z","shell.execute_reply.started":"2026-01-29T11:41:18.939808Z","shell.execute_reply":"2026-01-29T11:44:17.736612Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Loading processor...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f98f7a304b654b48ae22f2059500c4ff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"chat_template.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec7a7e1db9b949388ffec883e3f20d8c"}},"metadata":{}},{"name":"stderr","text":"The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. \n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1dbb821bb682413fbaaa20e933b785b4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"128aed89f4e04e38b8a4cacabcad1fcc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f973e91678b47cab3bedb882d82f793"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b080bcf05dc44103b7b0fbd0f1d78869"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"22346edfad934f578c491474dc6c04bf"}},"metadata":{}},{"name":"stdout","text":"Processor loaded!\n\nLoading model with 4-bit quantization...\nThis may take 3-5 minutes...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9fdc576a52fd42ef8d4247a8e1bc407b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (incomplete total...): 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d356cdb88aff4f29810688fdc19c2336"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9925f29b6a09496dbaba9ad028a53466"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading weights:   0%|          | 0/729 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23550c863a75451099ed6406806b5757"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/216 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2469e3361a11443c80f8138127d3537a"}},"metadata":{}},{"name":"stdout","text":"Model loaded!\nGPU Memory Used: 2.67 GB\n","output_type":"stream"}],"execution_count":6},{"id":"b044993c","cell_type":"markdown","source":"## 7. Configure LoRA","metadata":{}},{"id":"f9d6b2ea","cell_type":"code","source":"# Target modules for Qwen2.5-VL\n# These are the key attention layers\ntarget_modules = [\n    \"q_proj\",\n    \"k_proj\",\n    \"v_proj\",\n    \"o_proj\",\n    \"gate_proj\",\n    \"up_proj\",\n    \"down_proj\",\n]\n\n# LoRA Configuration\nlora_config = LoraConfig(\n    r=LORA_R,\n    lora_alpha=LORA_ALPHA,\n    target_modules=target_modules,\n    lora_dropout=LORA_DROPOUT,\n    bias=\"none\",\n    task_type=TaskType.CAUSAL_LM,\n)\n\n# Apply LoRA\nmodel = get_peft_model(model, lora_config)\n\n# Print trainable parameters\nmodel.print_trainable_parameters()\n\nprint(f\"\\nLoRA Configuration:\")\nprint(f\"  Rank (r): {LORA_R}\")\nprint(f\"  Alpha: {LORA_ALPHA}\")\nprint(f\"  Target modules: {target_modules}\")","metadata":{"execution":{"iopub.status.busy":"2026-01-29T11:44:17.738538Z","iopub.execute_input":"2026-01-29T11:44:17.739206Z","iopub.status.idle":"2026-01-29T11:44:19.167298Z","shell.execute_reply.started":"2026-01-29T11:44:17.739176Z","shell.execute_reply":"2026-01-29T11:44:19.166615Z"},"trusted":true},"outputs":[{"name":"stdout","text":"trainable params: 95,178,752 || all params: 8,387,345,408 || trainable%: 1.1348\n\nLoRA Configuration:\n  Rank (r): 32\n  Alpha: 64\n  Target modules: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']\n","output_type":"stream"}],"execution_count":7},{"id":"deaf9a74","cell_type":"markdown","source":"## 8. Create Training Prompts","metadata":{}},{"id":"619c8727","cell_type":"code","source":"SYSTEM_PROMPT = \"\"\"You are a precise error bar detection system for scientific plots.\nGiven an image of a scientific plot and data point coordinates, detect the error bars.\nFor each point, output the pixel distance from the data point to the top and bottom of the error bar.\nIf no error bar exists for a point, output 0 for both distances.\"\"\"\n\ndef create_input_prompt(input_points: List[Dict]) -> str:\n    \"\"\"\n    Create the input prompt with data point coordinates.\n    \"\"\"\n    points_str = json.dumps(input_points, indent=2)\n    \n    prompt = f\"\"\"Analyze this scientific plot image and detect error bars for the following data points:\n\n{points_str}\n\nFor each point, measure:\n- topBarPixelDistance: pixel distance from data point to top of error bar (0 if none)\n- bottomBarPixelDistance: pixel distance from data point to bottom of error bar (0 if none)\n\nOutput as JSON array:\n[\n  {{\"x\": <x>, \"y\": <y>, \"topBarPixelDistance\": <top>, \"bottomBarPixelDistance\": <bottom>}}\n]\"\"\"\n    \n    return prompt\n\ndef create_output_response(output_points: List[Dict]) -> str:\n    \"\"\"\n    Create the expected output response.\n    \"\"\"\n    return json.dumps(output_points, indent=2)\n\nprint(\"Prompt functions defined!\")\n\n# Test prompts\nif training_samples:\n    sample = training_samples[0]\n    print(\"\\nSample input prompt:\")\n    print(\"-\" * 60)\n    print(create_input_prompt(sample['input_points'][:2]))\n    print(\"-\" * 60)\n    print(\"\\nSample output response:\")\n    print(create_output_response(sample['output_points'][:2]))","metadata":{"execution":{"iopub.status.busy":"2026-01-29T11:44:19.168317Z","iopub.execute_input":"2026-01-29T11:44:19.168673Z","iopub.status.idle":"2026-01-29T11:44:19.176738Z","shell.execute_reply.started":"2026-01-29T11:44:19.168633Z","shell.execute_reply":"2026-01-29T11:44:19.175857Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Prompt functions defined!\n\nSample input prompt:\n------------------------------------------------------------\nAnalyze this scientific plot image and detect error bars for the following data points:\n\n[\n  {\n    \"x\": 177.0,\n    \"y\": 94.2\n  },\n  {\n    \"x\": 207.2,\n    \"y\": 105.5\n  }\n]\n\nFor each point, measure:\n- topBarPixelDistance: pixel distance from data point to top of error bar (0 if none)\n- bottomBarPixelDistance: pixel distance from data point to bottom of error bar (0 if none)\n\nOutput as JSON array:\n[\n  {\"x\": <x>, \"y\": <y>, \"topBarPixelDistance\": <top>, \"bottomBarPixelDistance\": <bottom>}\n]\n------------------------------------------------------------\n\nSample output response:\n[\n  {\n    \"x\": 177.0,\n    \"y\": 94.2,\n    \"topBarPixelDistance\": 100.7,\n    \"bottomBarPixelDistance\": 52.7\n  },\n  {\n    \"x\": 207.2,\n    \"y\": 105.5,\n    \"topBarPixelDistance\": 98.2,\n    \"bottomBarPixelDistance\": 39.9\n  }\n]\n","output_type":"stream"}],"execution_count":8},{"id":"0dd6f11a","cell_type":"markdown","source":"## 9. Prepare Dataset for Training","metadata":{}},{"id":"d026e91c","cell_type":"code","source":"def prepare_training_example(sample: Dict, processor) -> Dict:\n    \"\"\"\n    Prepare a single training example with image and text.\n    \"\"\"\n    # Load and resize image\n    image = Image.open(sample['image_path']).convert('RGB')\n    if max(image.size) > IMAGE_MAX_SIZE:\n        ratio = IMAGE_MAX_SIZE / max(image.size)\n        new_size = (int(image.size[0] * ratio), int(image.size[1] * ratio))\n        image = image.resize(new_size, Image.BILINEAR)\n    \n    # Create prompts\n    input_prompt = create_input_prompt(sample['input_points'])\n    output_response = create_output_response(sample['output_points'])\n    \n    # Create conversation format\n    messages = [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"image\", \"image\": image},\n                {\"type\": \"text\", \"text\": f\"{SYSTEM_PROMPT}\\n\\n{input_prompt}\"}\n            ]\n        },\n        {\n            \"role\": \"assistant\",\n            \"content\": output_response\n        }\n    ]\n    \n    # Apply chat template\n    text = processor.apply_chat_template(\n        messages,\n        tokenize=False,\n        add_generation_prompt=False\n    )\n    \n    return {\n        \"text\": text,\n        \"image\": image,\n        \"image_path\": sample['image_path']\n    }\n\n# Prepare training data\nprint(\"Preparing training examples...\")\nprepared_data = []\n\nfor i, sample in enumerate(tqdm(training_samples)):\n    try:\n        prepared = prepare_training_example(sample, processor)\n        prepared_data.append(prepared)\n    except Exception as e:\n        print(f\"Error preparing sample {i}: {e}\")\n        continue\n\nprint(f\"\\nPrepared {len(prepared_data)} training examples\")\n\n# Show sample\nif prepared_data:\n    print(f\"\\nSample text length: {len(prepared_data[0]['text'])} chars\")\n    print(f\"Sample text preview:\")\n    print(prepared_data[0]['text'][:500])","metadata":{"execution":{"iopub.status.busy":"2026-01-29T11:44:19.177776Z","iopub.execute_input":"2026-01-29T11:44:19.178037Z","iopub.status.idle":"2026-01-29T11:44:29.219205Z","shell.execute_reply.started":"2026-01-29T11:44:19.178016Z","shell.execute_reply":"2026-01-29T11:44:29.218445Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Preparing training examples...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 600/600 [00:10<00:00, 59.88it/s]","output_type":"stream"},{"name":"stdout","text":"\nPrepared 600 training examples\n\nSample text length: 4298 chars\nSample text preview:\n<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\n<|vision_start|><|image_pad|><|vision_end|>You are a precise error bar detection system for scientific plots.\nGiven an image of a scientific plot and data point coordinates, detect the error bars.\nFor each point, output the pixel distance from the data point to the top and bottom of the error bar.\nIf no error bar exists for a point, output 0 for both distances.\n\nAnalyze this scientific plot image and detect error bars for\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":9},{"id":"78a3b7b7","cell_type":"markdown","source":"## 10. Custom Training Loop (for VLM)","metadata":{}},{"id":"524920eb","cell_type":"code","source":"from torch.utils.data import DataLoader\nfrom torch.optim import AdamW\nfrom transformers import get_linear_schedule_with_warmup\nimport random\nfrom tqdm.auto import tqdm as tqdm_auto\n\ndef collate_fn(batch, processor):\n    \"\"\"\n    Custom collate function for VLM training.\n    \"\"\"\n    texts = [item['text'] for item in batch]\n    images = [item['image'] for item in batch]\n\n    inputs = processor(\n        text=texts,\n        images=images,\n        padding=True,\n        truncation=True,\n        max_length=MAX_SEQ_LENGTH,\n        return_tensors=\"pt\"\n    )\n\n    inputs['labels'] = inputs['input_ids'].clone()\n    return inputs\n\n\ndef train_vlm_lora(\n    model,\n    processor,\n    train_data,\n    num_epochs=NUM_EPOCHS,\n    batch_size=BATCH_SIZE,\n    learning_rate=LEARNING_RATE,\n    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n    save_steps=SAVE_STEPS,\n):\n    \"\"\"\n    Custom training loop for VLM with LoRA.\n    \"\"\"\n\n    random.shuffle(train_data)\n\n    optimizer = AdamW(model.parameters(), lr=learning_rate)\n\n    total_steps = (len(train_data) // batch_size) * num_epochs // gradient_accumulation_steps\n    warmup_steps = int(total_steps * WARMUP_RATIO)\n\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=warmup_steps,\n        num_training_steps=total_steps\n    )\n\n    print(\"\\nTraining Configuration:\")\n    print(f\"  Samples: {len(train_data)} | Epochs: {num_epochs} | Steps: {total_steps}\")\n    print(f\"  Batch: {batch_size} x {gradient_accumulation_steps} = {batch_size * gradient_accumulation_steps}\")\n    print(f\"  LR: {learning_rate} | Warmup: {warmup_steps} steps\")\n\n    model.train()\n    global_step = 0\n\n    epoch_progress = tqdm_auto(range(num_epochs), desc=\"Training Epochs\", position=0)\n\n    for epoch in epoch_progress:\n        print(\"\\n\" + \"=\" * 60)\n        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n        print(\"=\" * 60)\n\n        epoch_loss = 0.0\n        num_batches = 0\n\n        batch_progress = tqdm_auto(\n            range(0, len(train_data), batch_size),\n            desc=f\"Epoch {epoch + 1} Batches\",\n            position=1,\n            leave=False\n        )\n\n        for i in batch_progress:\n            batch = train_data[i:i + batch_size]\n\n            try:\n                inputs = collate_fn(batch, processor)\n                inputs = {k: v.to(model.device) for k, v in inputs.items()}\n\n                outputs = model(**inputs)\n                loss = outputs.loss / gradient_accumulation_steps\n                loss.backward()\n\n                epoch_loss += loss.item()\n                num_batches += 1\n\n                if num_batches % gradient_accumulation_steps == 0:\n                    optimizer.step()\n                    scheduler.step()\n                    optimizer.zero_grad()\n                    global_step += 1\n\n                    current_loss = epoch_loss / num_batches\n                    batch_progress.set_postfix({\n                        'loss': f'{current_loss:.4f}',\n                        'step': f'{global_step}/{total_steps}',\n                        'lr': f'{scheduler.get_last_lr()[0]:.2e}'\n                    })\n\n                    if global_step % LOGGING_STEPS == 0:\n                        print(\n                            f\"Step {global_step}/{total_steps} | \"\n                            f\"Loss: {current_loss:.4f} | \"\n                            f\"LR: {scheduler.get_last_lr()[0]:.2e}\"\n                        )\n\n                    if global_step % save_steps == 0:\n                        checkpoint_path = os.path.join(\n                            CHECKPOINT_DIR, f\"checkpoint-{global_step}\"\n                        )\n                        model.save_pretrained(checkpoint_path)\n                        print(f\"Saved checkpoint: {checkpoint_path}\")\n\n                del inputs, outputs, loss\n                if torch.cuda.is_available():\n                    torch.cuda.empty_cache()\n\n            except torch.cuda.OutOfMemoryError:\n                print(\"OOM encountered, skipping batch\")\n                optimizer.zero_grad()\n                if torch.cuda.is_available():\n                    torch.cuda.empty_cache()\n                continue\n\n        avg_epoch_loss = epoch_loss / num_batches if num_batches > 0 else 0.0\n        epoch_progress.set_postfix({'epoch_loss': f'{avg_epoch_loss:.4f}'})\n        print(f\"Epoch {epoch + 1} complete | Avg Loss: {avg_epoch_loss:.4f}\")\n\n    print(\"\\n\" + \"=\" * 60)\n    print(\"TRAINING COMPLETE\")\n    print(\"=\" * 60)\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2026-01-29T11:44:29.220240Z","iopub.execute_input":"2026-01-29T11:44:29.220494Z","iopub.status.idle":"2026-01-29T11:44:29.237099Z","shell.execute_reply.started":"2026-01-29T11:44:29.220471Z","shell.execute_reply":"2026-01-29T11:44:29.236263Z"},"trusted":true},"outputs":[],"execution_count":10},{"id":"65a878fc","cell_type":"markdown","source":"## 11. Train the Model","metadata":{}},{"id":"dbb8e02f","cell_type":"code","source":"# Train\nmodel = train_vlm_lora(\n    model=model,\n    processor=processor,\n    train_data=prepared_data,\n    num_epochs=NUM_EPOCHS,\n    batch_size=BATCH_SIZE,\n    learning_rate=LEARNING_RATE,\n    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n    save_steps=SAVE_STEPS,\n)","metadata":{"execution":{"iopub.status.busy":"2026-01-29T11:44:29.238105Z","iopub.execute_input":"2026-01-29T11:44:29.238344Z","iopub.status.idle":"2026-01-29T18:36:21.401760Z","shell.execute_reply.started":"2026-01-29T11:44:29.238319Z","shell.execute_reply":"2026-01-29T18:36:21.401038Z"},"trusted":true},"outputs":[{"name":"stdout","text":"\nTraining Configuration:\n  Samples: 600 | Epochs: 1 | Steps: 37\n  Batch: 1 x 16 = 16\n  LR: 0.0002 | Warmup: 1 steps\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training Epochs:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ba8c83e0e504a7b893be440b63c75e5"}},"metadata":{}},{"name":"stdout","text":"\n============================================================\nEpoch 1/1\n============================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 1 Batches:   0%|          | 0/600 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n","output_type":"stream"},{"name":"stdout","text":"Step 5/37 | Loss: 0.3896 | LR: 1.78e-04\nStep 10/37 | Loss: 0.2972 | LR: 1.50e-04\nStep 15/37 | Loss: 0.2567 | LR: 1.22e-04\nStep 20/37 | Loss: 0.2336 | LR: 9.44e-05\nStep 25/37 | Loss: 0.2189 | LR: 6.67e-05\nStep 30/37 | Loss: 0.2084 | LR: 3.89e-05\nSaved checkpoint: /kaggle/working/qwen_vl_checkpoints/checkpoint-30\nStep 35/37 | Loss: 0.2009 | LR: 1.11e-05\nEpoch 1 complete | Avg Loss: 0.1983\n\n============================================================\nTRAINING COMPLETE\n============================================================\n","output_type":"stream"}],"execution_count":11},{"id":"d6fd8c17","cell_type":"markdown","source":"## 12. Save Final Model","metadata":{}},{"id":"e61fd591","cell_type":"code","source":"# Save final LoRA adapter\nfinal_checkpoint_path = os.path.join(CHECKPOINT_DIR, \"final_lora_adapter\")\n\nprint(f\"Saving LoRA adapter to: {final_checkpoint_path}\")\nmodel.save_pretrained(final_checkpoint_path)\nprocessor.save_pretrained(final_checkpoint_path)\n\nprint(f\"\\nCheckpoint saved!\")\nprint(f\"Contents:\")\nfor f in os.listdir(final_checkpoint_path):\n    size = os.path.getsize(os.path.join(final_checkpoint_path, f)) / 1024\n    print(f\"  - {f} ({size:.1f} KB)\")","metadata":{"execution":{"iopub.status.busy":"2026-01-29T18:36:21.402802Z","iopub.execute_input":"2026-01-29T18:36:21.403070Z","iopub.status.idle":"2026-01-29T18:36:23.305288Z","shell.execute_reply.started":"2026-01-29T18:36:21.403039Z","shell.execute_reply":"2026-01-29T18:36:23.304444Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Saving LoRA adapter to: /kaggle/working/qwen_vl_checkpoints/final_lora_adapter\n\nCheckpoint saved!\nContents:\n  - adapter_model.safetensors (371875.5 KB)\n  - tokenizer.json (11154.5 KB)\n  - adapter_config.json (1.0 KB)\n  - chat_template.jinja (1.0 KB)\n  - tokenizer_config.json (0.7 KB)\n  - processor_config.json (1.4 KB)\n  - README.md (5.1 KB)\n","output_type":"stream"}],"execution_count":12},{"id":"e6e73af3","cell_type":"markdown","source":"## 13. Upload to HuggingFace Hub","metadata":{}},{"id":"ce1638fe","cell_type":"code","source":"# Create repository\napi = HfApi()\n\ntry:\n    create_repo(\n        repo_id=HF_REPO_ID,\n        repo_type=\"model\",\n        private=False,\n        exist_ok=True,\n    )\n    print(f\"Repository created/verified: {HF_REPO_ID}\")\nexcept Exception as e:\n    print(f\"Repo note: {e}\")\n\n# Push to Hub\nprint(f\"\\nUploading to HuggingFace Hub: {HF_REPO_ID}\")\n\nmodel.push_to_hub(\n    repo_id=HF_REPO_ID,\n    commit_message=\"Upload fine-tuned Qwen2.5-VL error bar detector\",\n)\n\nprocessor.push_to_hub(\n    repo_id=HF_REPO_ID,\n    commit_message=\"Upload processor\",\n)\n\nprint(f\"\\n{'='*60}\")\nprint(f\"MODEL UPLOADED: https://huggingface.co/{HF_REPO_ID}\")\nprint(f\"{'='*60}\")","metadata":{"execution":{"iopub.status.busy":"2026-01-29T18:36:23.306512Z","iopub.execute_input":"2026-01-29T18:36:23.307130Z","iopub.status.idle":"2026-01-29T18:36:37.251328Z","shell.execute_reply.started":"2026-01-29T18:36:23.307104Z","shell.execute_reply":"2026-01-29T18:36:37.250621Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Repository created/verified: Sayeem26s/Chartqwen\n\nUploading to HuggingFace Hub: Sayeem26s/Chartqwen\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef8218a55a1a4caf87b37a037456de98"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processing Files (0 / 0): |          |  0.00B /  0.00B            ","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0631121ea99a422ba6797abacba0ceff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"New Data Upload: |          |  0.00B /  0.00B            ","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3481739162bc4d5da4937007ef31056c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17f4962265014347968d24ebfffac57e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processing Files (0 / 0): |          |  0.00B /  0.00B            ","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6dd2e1ea29a54b8f8f410652bb28360a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"New Data Upload: |          |  0.00B /  0.00B            ","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c60f6b03cb5c4d87b629b3049e93ffe0"}},"metadata":{}},{"name":"stdout","text":"\n============================================================\nMODEL UPLOADED: https://huggingface.co/Sayeem26s/Chartqwen\n============================================================\n","output_type":"stream"}],"execution_count":13},{"id":"07f769fb","cell_type":"markdown","source":"## Summary\n\nThis notebook demonstrated:\n\n### Fine-tuning:\n- **Model**: Qwen2.5-VL-7B-Instruct (Vision-Language Model)\n- **Method**: LoRA with 4-bit quantization\n- **Task**: Error bar detection in scientific plots\n- **Input**: Image + data point coordinates (x, y)\n- **Output**: Error bar distances (topBarPixelDistance, bottomBarPixelDistance)\n\n### Data Format:\n- **Training Labels**: JSON with `points` array containing `x`, `y`, `topBarPixelDistance`, `bottomBarPixelDistance`\n- **Test Input**: JSON with `image_file` and `data_points` (x, y only)\n- **Test Ground Truth**: Same format as training labels\n\n### Key Features:\n- Custom training loop for VLM with image inputs\n- Memory-efficient training with gradient accumulation\n- Checkpointing and HuggingFace Hub upload\n- Comprehensive evaluation metrics\n\n### Model Path:\n- HuggingFace: `Sayeem26s/qwen-vl-errorbar-detector`","metadata":{}}]}